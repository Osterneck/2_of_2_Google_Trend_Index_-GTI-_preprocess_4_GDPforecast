{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#GTI-4-GDP - Part 2-of-2\n",
        "#Part 1 = Google Trends Index (GTI) Pipeline (notebook1)\n",
        "#(used as input 1-of-16 to THIS notebook2:\n",
        "\n",
        "#Part 2-of-2, is 16 time-series input to create GDP NowCast Forecast\n",
        "#Objective: Forecast quarterly U.S. GDP before the official-release."
      ],
      "metadata": {
        "id": "3C8_X9y9Bxe7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Cell 0A: Automated Economic Data Fetcher and Harmonizer.\n",
        "\n",
        "This cell automates the multi-day process of fetching, cleaning, and aligning\n",
        "economic data from disparate sources into a unified quarterly dataset.\n",
        "\n",
        "TWO FORECAST MODES:\n",
        "===================\n",
        "\n",
        "MODE 1: STATIC (Recommended - Maximum Accuracy)\n",
        "-----------------------------------------------\n",
        "Use for: Forecasting last COMPLETE quarter before BEA release\n",
        "Timing: Run ~7 days before BEA advance estimate\n",
        "Data: ALL data ABOUT last complete quarter (includes lagged releases)\n",
        "Accuracy: MAXIMUM (matches model training format)\n",
        "\n",
        "Example Timeline:\n",
        "  Dec 31, 2024: Q4 ends\n",
        "  Jan 1-20, 2025: Economic reports release (Dec data)\n",
        "  Jan 21, 2025: YOU run static mode forecast for Q4\n",
        "  Jan 28, 2025: BEA releases Q4 GDP\n",
        "  Your advantage: 7-day head start, same data quality\n",
        "\n",
        "Why Static is Most Accurate:\n",
        "  ✓ Model trained on COMPLETE quarterly data (all 90 days)\n",
        "  ✓ Static mode uses COMPLETE quarters (all 90 days)\n",
        "  ✓ Apples-to-apples comparison = maximum accuracy\n",
        "  ✓ All 16 time-series complete and aligned\n",
        "\n",
        "MODE 2: DYNAMIC (Experimental - Real-Time Nowcast)\n",
        "--------------------------------------------------\n",
        "Use for: Real-time monitoring of CURRENT quarter\n",
        "Timing: Run anytime (weekly, daily)\n",
        "Data: Includes PARTIAL current quarter data\n",
        "Accuracy: LOWER (incomplete data, format mismatch)\n",
        "\n",
        "Example Timeline:\n",
        "  Jan 21, 2025: Mid-Q1 (quarter ends Mar 31)\n",
        "  YOU run dynamic mode: Forecast Q1 using 21 days of data\n",
        "  Warning: Using 21-day average instead of 90-day average\n",
        "\n",
        "Why Dynamic is Less Accurate:\n",
        "  ⚠ Model trained on 90-day quarterly averages\n",
        "  ⚠ Dynamic uses partial quarters (e.g., 21-day averages)\n",
        "  ⚠ Format mismatch reduces prediction accuracy\n",
        "  ⚠ Some time-series may have incomplete/outdated data\n",
        "\n",
        "When to Use Dynamic:\n",
        "  - Weekly GDP trend monitoring\n",
        "  - Early warning signals\n",
        "  - Experimental nowcasting\n",
        "  - Accept lower accuracy for timeliness\n",
        "\n",
        "CRITICAL METHODOLOGY - STATIC MODE (A2):\n",
        "========================================\n",
        "\n",
        "SCENARIO: Forecasting Q4 2024 GDP on January 21, 2025\n",
        "- Quarter ended: December 31, 2024\n",
        "- BEA announces: January 28, 2025 (7 days later)\n",
        "- YOUR ADVANTAGE: 7-day head start\n",
        "\n",
        "DATA CUTOFF APPROACH:\n",
        "- Fetches ALL data ABOUT Q4 2024 that is AVAILABLE BY January 21\n",
        "- Includes December economic reports released in early January\n",
        "- Example: December unemployment rate (released ~Jan 10) IS included\n",
        "- Example: Partial January data (incomplete Q1) is NOT included\n",
        "\n",
        "WHY THIS IS MOST ACCURATE:\n",
        "- Your model was TRAINED on complete quarterly data (all 90 days)\n",
        "- Testing must match training format for valid predictions\n",
        "- Using partial quarter data violates training assumptions\n",
        "- Complete Q4 data = apples-to-apples comparison\n",
        "\n",
        "Data Sources:\n",
        "- BEA API: GDP (primary source)\n",
        "- FRED API: 10-year yield, gas prices, jobless claims, corporate yields,\n",
        "  unemployment rate\n",
        "- Yahoo Finance: VIX volatility index\n",
        "- User Upload: GTI from Notebook 1\n",
        "\n",
        "Frequency Harmonization:\n",
        "- Daily data (yield, VIX, corp bonds) → Quarterly average\n",
        "- Weekly data (gas, jobless claims) → Quarterly average\n",
        "- Monthly data (unemployment) → Quarterly average\n",
        "- Quarterly data (GDP) → Already aligned\n",
        "- GTI (quarterly) → Merge on date\n",
        "\n",
        "Output: 5TS_A_NO_LEAK.xlsx (ready for GDP forecasting pipeline)\n",
        "\n",
        "Author: Alex Osterneck, CLA, MSCS\n",
        "Organization: ai70000, Ltd.\n",
        "\"\"\"\n",
        "\n",
        "import io\n",
        "import subprocess\n",
        "import sys\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "from typing import Any, Dict, Optional, Tuple\n",
        "\n",
        "import ipywidgets as widgets\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "from IPython.display import clear_output, display\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "# Configuration constants.\n",
        "START_DATE = '2004-01-01'\n",
        "QUARTERLY_FREQ = 'QS'  # Quarter start frequency.\n",
        "FRED_BASE_URL = 'https://fred.stlouisfed.org/graph/fredgraph.csv'\n",
        "\n",
        "\n",
        "# Module-level variables.\n",
        "gti_data: Optional[pd.DataFrame] = None\n",
        "economic_data: Optional[pd.DataFrame] = None\n",
        "forecast_mode: str = 'static'  # 'static' or 'dynamic'\n",
        "\n",
        "\n",
        "def install_dependencies() -> None:\n",
        "    \"\"\"Install required packages for data fetching.\n",
        "\n",
        "    Checks if yfinance and pandas-datareader are installed.\n",
        "    If not, installs them via pip.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Attempt to import required packages.\n",
        "        # If already installed, these imports will succeed silently.\n",
        "        import yfinance\n",
        "        import pandas_datareader\n",
        "    except ImportError:\n",
        "        # One or both packages missing - install them.\n",
        "        print(\"Installing required packages (yfinance, pandas-datareader)...\")\n",
        "\n",
        "        # Use subprocess to call pip install.\n",
        "        # -m pip: Ensures we use pip from current Python environment.\n",
        "        # install -q: Quiet mode (minimal output).\n",
        "        subprocess.check_call([\n",
        "            sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n",
        "            \"yfinance\", \"pandas-datareader\"\n",
        "        ])\n",
        "\n",
        "        print(\"Installation complete.\")\n",
        "\n",
        "\n",
        "def fetch_gdp_from_bea() -> Optional[pd.DataFrame]:\n",
        "    \"\"\"Fetch GDP directly from BEA API.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with quarterly GDP data, or None if fetch fails.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        url = \"https://apps.bea.gov/api/data\"\n",
        "\n",
        "        params = {\n",
        "            'UserID': 'DEMO_KEY',\n",
        "            'method': 'GetData',\n",
        "            'datasetname': 'NIPA',\n",
        "            'TableName': 'T10101',\n",
        "            'Frequency': 'Q',\n",
        "            'Year': 'ALL',\n",
        "            'ResultFormat': 'json'\n",
        "        }\n",
        "\n",
        "        response = requests.get(url, params=params, timeout=30)\n",
        "        data = response.json()\n",
        "\n",
        "        if 'BEAAPI' in data and 'Results' in data['BEAAPI']:\n",
        "            observations = data['BEAAPI']['Results']['Data']\n",
        "\n",
        "            records = []\n",
        "            for obs in observations:\n",
        "                if obs.get('LineNumber') == '1':\n",
        "                    period = obs.get('TimePeriod', '')\n",
        "                    value = obs.get('DataValue', '')\n",
        "\n",
        "                    if period and value and value != '...':\n",
        "                        year = int(period[:4])\n",
        "                        q = int(period[-1])\n",
        "                        month = (q - 1) * 3 + 1\n",
        "                        date = pd.Timestamp(year, month, 1)\n",
        "\n",
        "                        gdp = float(value.replace(',', ''))\n",
        "                        records.append({'date': date, 'GDP': gdp})\n",
        "\n",
        "            if records:\n",
        "                df = pd.DataFrame(records).set_index('date')\n",
        "                return df[df.index >= START_DATE]\n",
        "\n",
        "        return None\n",
        "\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "\n",
        "def fetch_fred_series(\n",
        "    series_id: str,\n",
        "    start_date: str = START_DATE\n",
        ") -> Optional[pd.DataFrame]:\n",
        "    \"\"\"Fetch time series data from FRED.\n",
        "\n",
        "    Uses direct CSV download (no API key required).\n",
        "    Falls back to pandas-datareader if CSV method fails.\n",
        "    Handles multiple possible date column names from FRED.\n",
        "\n",
        "    Args:\n",
        "        series_id: FRED series identifier (e.g., 'DGS10').\n",
        "        start_date: Start date in YYYY-MM-DD format.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with date index and values, or None if fetch fails.\n",
        "    \"\"\"\n",
        "    # METHOD 1: Try direct CSV download first.\n",
        "    try:\n",
        "        # Construct FRED CSV download URL.\n",
        "        # Format: base_url?id=SERIES&cosd=START_DATE\n",
        "        # No end date - let FRED return all available data\n",
        "        url = f\"{FRED_BASE_URL}?id={series_id}&cosd={start_date}\"\n",
        "\n",
        "        # Download CSV directly from FRED without parsing dates first.\n",
        "        # This avoids errors if date column has unexpected name.\n",
        "        df = pd.read_csv(url)\n",
        "\n",
        "        # Detect date column name (FRED sometimes uses different names).\n",
        "        # Common variations: DATE, date, observation_date, Date\n",
        "        date_col = None\n",
        "        for possible_name in ['DATE', 'date', 'observation_date', 'Date']:\n",
        "            if possible_name in df.columns:\n",
        "                date_col = possible_name\n",
        "                break\n",
        "\n",
        "        # If no date column found, check first column.\n",
        "        if date_col is None:\n",
        "            # Assume first column is the date.\n",
        "            date_col = df.columns[0]\n",
        "\n",
        "        # Convert date column to datetime.\n",
        "        df[date_col] = pd.to_datetime(df[date_col])\n",
        "\n",
        "        # Set date as index.\n",
        "        df = df.set_index(date_col)\n",
        "\n",
        "        # Get value column (should be second column or series_id).\n",
        "        value_col = None\n",
        "        for col in df.columns:\n",
        "            if col != date_col:\n",
        "                value_col = col\n",
        "                break\n",
        "\n",
        "        # If value column not found, use first remaining column.\n",
        "        if value_col is None and len(df.columns) > 0:\n",
        "            value_col = df.columns[0]\n",
        "\n",
        "        # Extract just the value column.\n",
        "        if value_col is not None:\n",
        "            df = df[[value_col]]\n",
        "            df.columns = [series_id]\n",
        "        else:\n",
        "            # Malformed response - try fallback method.\n",
        "            raise ValueError(\"No value column found in CSV response\")\n",
        "\n",
        "        # Replace FRED's missing value indicator '.' with NaN.\n",
        "        # FRED uses '.' to denote missing/unavailable data points.\n",
        "        df = df.replace('.', np.nan)\n",
        "\n",
        "        # Convert all values to float type.\n",
        "        # Some FRED series may come as strings with '.' values.\n",
        "        df = df.astype(float)\n",
        "\n",
        "        return df\n",
        "\n",
        "    except Exception as csv_error:\n",
        "        # METHOD 2: Fall back to pandas_datareader.\n",
        "        try:\n",
        "            from pandas_datareader import data as pdr\n",
        "\n",
        "            # No end date - let pandas_datareader get all available data\n",
        "            df = pdr.DataReader(series_id, 'fred', start_date)\n",
        "\n",
        "            # Ensure column is named with series_id.\n",
        "            if len(df.columns) == 1 and df.columns[0] != series_id:\n",
        "                df.columns = [series_id]\n",
        "\n",
        "            return df\n",
        "\n",
        "        except Exception as pdr_error:\n",
        "            # Both methods failed - log and return None.\n",
        "            print(f\"Warning: Could not fetch {series_id}\")\n",
        "            print(f\"  CSV method error: {csv_error}\")\n",
        "            print(f\"  DataReader error: {pdr_error}\")\n",
        "            return None\n",
        "\n",
        "\n",
        "def fetch_vix_data(start_date: str = START_DATE) -> Optional[pd.DataFrame]:\n",
        "    \"\"\"Fetch VIX volatility index from Yahoo Finance.\n",
        "\n",
        "    Args:\n",
        "        start_date: Start date in YYYY-MM-DD format.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with date index and VIX closing prices, or None if fails.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Import yfinance library for Yahoo Finance API access.\n",
        "        import yfinance as yf\n",
        "\n",
        "        # Download VIX (^VIX ticker symbol) historical data.\n",
        "        # progress=False: Suppress download progress bar.\n",
        "        # No end date - let yfinance get all available data through today\n",
        "        vix = yf.download('^VIX', start=start_date, progress=False)\n",
        "\n",
        "        # Check if download returned empty dataframe.\n",
        "        if vix is None or vix.empty:\n",
        "            return None\n",
        "\n",
        "        # Handle multi-index columns from yfinance.\n",
        "        if isinstance(vix.columns, pd.MultiIndex):\n",
        "            vix.columns = vix.columns.droplevel(1)\n",
        "\n",
        "        # Extract closing price column only.\n",
        "        # VIX dataframe includes: Open, High, Low, Close, Volume, Adj Close.\n",
        "        # We use Close for end-of-day VIX level.\n",
        "        if 'Close' in vix.columns:\n",
        "            df = pd.DataFrame({'VIX': vix['Close']})\n",
        "        elif 'Adj Close' in vix.columns:\n",
        "            df = pd.DataFrame({'VIX': vix['Adj Close']})\n",
        "        else:\n",
        "            # Unknown column structure.\n",
        "            return None\n",
        "\n",
        "        return df\n",
        "\n",
        "    except Exception as error:\n",
        "        # Log fetch failure and return None.\n",
        "        print(f\"Warning: Could not fetch VIX data: {error}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def resample_to_quarterly(\n",
        "    data: pd.DataFrame,\n",
        "    method: str = 'mean'\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Resample time series data to quarterly frequency.\n",
        "\n",
        "    CRITICAL: Uses forward-fill before aggregation to ensure complete\n",
        "    quarterly values that match training data format.\n",
        "\n",
        "    Why forward-fill?\n",
        "    - Daily/weekly data has gaps (weekends, holidays, reporting delays)\n",
        "    - Without forward-fill: gaps become NaN, quarterly avg becomes NaN\n",
        "    - With forward-fill: gaps use last known value (realistic assumption)\n",
        "    - Result: Complete quarterly average over all 90 days\n",
        "\n",
        "    Example (10-year yield):\n",
        "      Jan 1 (Mon): 4.05\n",
        "      Jan 2 (Tue): Holiday, no data\n",
        "      Jan 3 (Wed): 4.07\n",
        "\n",
        "      Without forward-fill: Q1 avg includes NaN → fails\n",
        "      With forward-fill: Jan 2 = 4.05 → Q1 avg = (4.05+4.05+4.07)/3\n",
        "\n",
        "    This matches how financial markets work: rates don't disappear on\n",
        "    weekends, they just aren't updated. Forward-fill reflects reality.\n",
        "\n",
        "    A2 METHODOLOGY:\n",
        "    - Fetches all available data (includes lagged monthly releases)\n",
        "    - Forward-fills gaps to create complete time series\n",
        "    - Aggregates to quarterly (mean/last/sum)\n",
        "    - Result: Complete quarterly data matching training format\n",
        "\n",
        "    Args:\n",
        "        data: DataFrame with datetime index and numeric columns.\n",
        "        method: Aggregation method ('mean', 'last', 'sum').\n",
        "\n",
        "    Returns:\n",
        "        DataFrame resampled to quarterly frequency (QS).\n",
        "    \"\"\"\n",
        "    # Forward fill missing values before aggregation.\n",
        "    # This is ESSENTIAL for A2 methodology - ensures complete quarters.\n",
        "    data_filled = data.fillna(method='ffill')\n",
        "\n",
        "    # Resample to quarter start.\n",
        "    if method == 'mean':\n",
        "        quarterly = data_filled.resample(QUARTERLY_FREQ).mean()\n",
        "    elif method == 'last':\n",
        "        quarterly = data_filled.resample(QUARTERLY_FREQ).last()\n",
        "    elif method == 'sum':\n",
        "        quarterly = data_filled.resample(QUARTERLY_FREQ).sum()\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown aggregation method: {method}\")\n",
        "\n",
        "    return quarterly\n",
        "\n",
        "\n",
        "def align_to_common_dates(\n",
        "    dataframes: Dict[str, pd.DataFrame]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Align multiple quarterly dataframes to common date index.\n",
        "\n",
        "    Takes intersection of dates across all series to ensure complete\n",
        "    data availability for all features. This prevents downstream\n",
        "    errors from missing values in any indicator.\n",
        "\n",
        "    Args:\n",
        "        dataframes: Dictionary mapping column names to DataFrames.\n",
        "\n",
        "    Returns:\n",
        "        Single DataFrame with all series aligned to common dates.\n",
        "    \"\"\"\n",
        "    # Create empty dataframe to combine all series.\n",
        "    combined = pd.DataFrame()\n",
        "\n",
        "    # Iterate through each dataframe and add to combined.\n",
        "    for name, df in dataframes.items():\n",
        "        # Validate dataframe is not None and not empty.\n",
        "        if df is not None and not df.empty:\n",
        "            # Extract first column (series values) and add to combined.\n",
        "            # Use column name from dictionary key for clarity.\n",
        "            combined[name] = df.iloc[:, 0]\n",
        "\n",
        "    # Drop any rows with missing values (NaN).\n",
        "    # This takes the intersection of dates where ALL series have data.\n",
        "    # Result: Only complete quarters with all 7 indicators present.\n",
        "    aligned = combined.dropna()\n",
        "\n",
        "    return aligned\n",
        "\n",
        "\n",
        "def fetch_all_economic_indicators() -> Optional[pd.DataFrame]:\n",
        "    \"\"\"Fetch and harmonize all economic indicators.\n",
        "\n",
        "    Implements mode-specific data filtering:\n",
        "\n",
        "    STATIC MODE (forecast_mode='static'):\n",
        "    - Fetches ALL data ABOUT last complete quarter\n",
        "    - Filters out current incomplete quarter (7-day cutoff)\n",
        "    - Includes lagged releases (e.g., Dec unemployment in early Jan)\n",
        "    - Result: Complete quarterly data matching training format\n",
        "    - Maximum accuracy for BEA comparison\n",
        "\n",
        "    DYNAMIC MODE (forecast_mode='dynamic'):\n",
        "    - Fetches ALL available data including current partial quarter\n",
        "    - Includes incomplete current quarter for nowcasting\n",
        "    - Warning: Partial quarters reduce accuracy\n",
        "    - Result: Real-time trend monitoring, not precision forecast\n",
        "\n",
        "    FRED automatically provides latest available data for each series,\n",
        "    which naturally implements the complete data approach.\n",
        "\n",
        "    Retrieves data from FRED and Yahoo Finance, resamples to quarterly\n",
        "    frequency, and aligns to common date index.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with 7 economic indicators at quarterly frequency,\n",
        "        or None if critical data unavailable.\n",
        "    \"\"\"\n",
        "    # Declare global variable access FIRST (before any usage).\n",
        "    global forecast_mode\n",
        "\n",
        "    # Display mode-specific header messages.\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    if forecast_mode == 'static':\n",
        "        print(\"STATIC MODE: Fetching Complete Quarterly Data\")\n",
        "        print(\"=\" * 70)\n",
        "        print(\"Fetching ALL data about last complete quarter...\")\n",
        "        print(\"Includes lagged releases for maximum accuracy.\")\n",
        "    else:\n",
        "        print(\"DYNAMIC MODE: Fetching Latest Available Data\")\n",
        "        print(\"=\" * 70)\n",
        "        print(\"Fetching all available data including current quarter...\")\n",
        "        print(\"WARNING: Current quarter may be incomplete.\")\n",
        "\n",
        "    print(\"\")\n",
        "\n",
        "    # Define FRED series identifiers and descriptions.\n",
        "    fred_series = {\n",
        "        'DGS10': '10-Year Treasury Yield',\n",
        "        'GASREGW': 'Regular Gas Price',\n",
        "        'ICSA': 'Initial Jobless Claims',\n",
        "        'BAA': 'Corporate Bond Yield (BAA)',\n",
        "        'UNRATE': 'Unemployment Rate'\n",
        "    }\n",
        "\n",
        "    # Fetch FRED data series by series.\n",
        "    # Dictionary to store fetched dataframes.\n",
        "    fred_data = {}\n",
        "\n",
        "    for series_id, description in fred_series.items():\n",
        "        print(f\"Fetching {description} ({series_id})...\", end=' ')\n",
        "        data = fetch_fred_series(series_id)\n",
        "\n",
        "        if data is not None:\n",
        "            print(f\"Success ({len(data)} records)\")\n",
        "            fred_data[series_id] = data\n",
        "        else:\n",
        "            print(\"Failed\")\n",
        "            # Critical data missing - abort.\n",
        "            return None\n",
        "\n",
        "    # Fetch GDP from BEA (primary source), fall back to FRED if needed.\n",
        "    print(\"Fetching GDP from BEA (primary source)...\", end=' ')\n",
        "    gdp_data = fetch_gdp_from_bea()\n",
        "\n",
        "    if gdp_data is None:\n",
        "        # Fall back to FRED\n",
        "        print(\"Failed - trying FRED...\", end=' ')\n",
        "        gdp_data = fetch_fred_series('GDP')\n",
        "\n",
        "    if gdp_data is not None:\n",
        "        print(f\"Success ({len(gdp_data)} records)\")\n",
        "        fred_data['GDP'] = gdp_data\n",
        "    else:\n",
        "        print(\"Failed\")\n",
        "        return None\n",
        "\n",
        "    # Fetch VIX data from Yahoo Finance.\n",
        "    print(\"Fetching VIX from Yahoo Finance...\", end=' ')\n",
        "    vix_data = fetch_vix_data()\n",
        "\n",
        "    if vix_data is not None:\n",
        "        print(f\"Success ({len(vix_data)} records)\")\n",
        "    else:\n",
        "        print(\"Failed\")\n",
        "        # VIX is critical - abort if unavailable.\n",
        "        return None\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Resampling all series to quarterly frequency...\")\n",
        "\n",
        "    # Resample each series from native frequency to quarterly.\n",
        "    # Dictionary to store quarterly-resampled dataframes.\n",
        "    quarterly_data = {}\n",
        "\n",
        "    # Daily, weekly, and monthly series → quarterly average.\n",
        "    # Use mean aggregation to get representative quarterly value.\n",
        "    for series_id in ['DGS10', 'GASREGW', 'ICSA', 'BAA', 'UNRATE']:\n",
        "        quarterly_data[series_id] = resample_to_quarterly(\n",
        "            fred_data[series_id], method='mean'\n",
        "        )\n",
        "\n",
        "    # VIX volatility index → quarterly average.\n",
        "    quarterly_data['VIX'] = resample_to_quarterly(vix_data, method='mean')\n",
        "\n",
        "    # GDP is already quarterly - just align to quarter start dates.\n",
        "    # Use 'last' to take the quarterly value as-is.\n",
        "    quarterly_data['GDP'] = resample_to_quarterly(\n",
        "        fred_data['GDP'], method='last'\n",
        "    )\n",
        "\n",
        "    # DIAGNOSTIC: Show what GDP data was actually provided.\n",
        "    print(\"\")\n",
        "    print(\"DIAGNOSTIC - GDP Data Range:\")\n",
        "    if quarterly_data['GDP'] is not None and not quarterly_data['GDP'].empty:\n",
        "        gdp_df = quarterly_data['GDP']\n",
        "        print(f\"  Start: {gdp_df.index[0].date()}\")\n",
        "        print(f\"  End: {gdp_df.index[-1].date()}\")\n",
        "        print(f\"  Quarters: {len(gdp_df)}\")\n",
        "        print(f\"  Last value: {gdp_df.iloc[-1, 0]:.2f}\")\n",
        "    else:\n",
        "        print(\"  ERROR: No GDP data received\")\n",
        "    print(\"\")\n",
        "\n",
        "    print(\"Aligning all series to common dates...\")\n",
        "\n",
        "    # Combine all quarterly data.\n",
        "    # Both modes: Start by combining all data (allow NaN for now).\n",
        "    combined = pd.DataFrame()\n",
        "    for name, df in quarterly_data.items():\n",
        "        if df is not None and not df.empty:\n",
        "            combined[name] = df.iloc[:, 0]\n",
        "\n",
        "    # MODE-SPECIFIC FILTERING.\n",
        "    if forecast_mode == 'static':\n",
        "        # STATIC MODE: Require complete data for forecasting.\n",
        "        # Drop rows where GDP is missing (can't forecast without target).\n",
        "        # But show most recent complete GDP data available.\n",
        "        aligned = combined.dropna(subset=['GDP'])\n",
        "\n",
        "        print(\"\")\n",
        "        print(\"STATIC MODE: Using quarters with complete GDP data\")\n",
        "        if len(aligned) > 0:\n",
        "            print(f\"Latest GDP data through: {aligned.index[-1].date()}\")\n",
        "            print(f\"Total quarters: {len(aligned)}\")\n",
        "            print(f\"Last GDP value: {aligned['GDP'].iloc[-1]:.2f}\")\n",
        "            print(\"\")\n",
        "            print(\"Last 3 quarters of data:\")\n",
        "            display_df = aligned.tail(3).round(2)\n",
        "            print(display_df.to_string())\n",
        "    else:\n",
        "        # DYNAMIC MODE: Keep all data, even if GDP missing.\n",
        "        aligned = combined\n",
        "        print(\"\")\n",
        "        print(\"DYNAMIC MODE: Including all available data\")\n",
        "        if len(aligned) > 0:\n",
        "            print(f\"Data through: {aligned.index[-1].date()}\")\n",
        "            print(f\"Total quarters: {len(aligned)}\")\n",
        "            # Check for missing GDP.\n",
        "            gdp_missing = aligned['GDP'].isna().sum()\n",
        "            if gdp_missing > 0:\n",
        "                print(f\"⚠ {gdp_missing} quarters with missing GDP\")\n",
        "            print(\"\")\n",
        "            print(\"Last 3 quarters of data:\")\n",
        "            display_df = aligned.tail(3).round(2)\n",
        "            print(display_df.to_string())\n",
        "\n",
        "    # Rename columns from FRED codes to descriptive names.\n",
        "    # This matches the expected format for downstream processing.\n",
        "    aligned = aligned.rename(columns={\n",
        "        'DGS10': 'yield_10yr',\n",
        "        'GASREGW': 'gas_price',\n",
        "        'ICSA': 'jobless_claims_quarterly_avg',\n",
        "        'VIX': 'vix_quarterly_avg',\n",
        "        'BAA': 'CorpYield_QtrAvg',\n",
        "        'UNRATE': 'UNRATE_QtrAvg',\n",
        "        'GDP': 'gdp'\n",
        "    })\n",
        "\n",
        "    # Calculate GDP percent change (the target variable).\n",
        "    # Formula: ((GDP[t] - GDP[t-1]) / GDP[t-1]) * 100\n",
        "    aligned['gdp_pct_change_target'] = aligned['gdp'].pct_change() * 100\n",
        "\n",
        "    # Display summary statistics.\n",
        "    print(f\"Success: {len(aligned)} quarterly periods aligned\")\n",
        "    print(f\"Date range: {aligned.index[0].date()} to {aligned.index[-1].date()}\")\n",
        "    print(\"\")\n",
        "\n",
        "    return aligned\n",
        "\n",
        "\n",
        "def create_mode_selector() -> None:\n",
        "    \"\"\"Create mode selector for static vs dynamic forecasting.\"\"\"\n",
        "    global forecast_mode\n",
        "\n",
        "    # Create buttons instead of radio buttons for cleaner layout.\n",
        "    btn_static = widgets.Button(\n",
        "        description='STATIC Mode',\n",
        "        button_style='info',\n",
        "        tooltip='Maximum accuracy - recommended',\n",
        "        layout=widgets.Layout(width='200px')\n",
        "    )\n",
        "\n",
        "    btn_dynamic = widgets.Button(\n",
        "        description='DYNAMIC Mode',\n",
        "        button_style='',\n",
        "        tooltip='Real-time nowcast - experimental',\n",
        "        layout=widgets.Layout(width='200px')\n",
        "    )\n",
        "\n",
        "    # Info display area.\n",
        "    info_output = widgets.Output()\n",
        "\n",
        "    def show_static_info(b: widgets.Button) -> None:\n",
        "        \"\"\"Display static mode information.\n",
        "\n",
        "        Args:\n",
        "            b: Button widget that triggered event.\n",
        "        \"\"\"\n",
        "        global forecast_mode\n",
        "        forecast_mode = 'static'\n",
        "\n",
        "        # Update button styles.\n",
        "        btn_static.button_style = 'info'\n",
        "        btn_dynamic.button_style = ''\n",
        "\n",
        "        with info_output:\n",
        "            clear_output()\n",
        "            print(\"STATIC MODE SELECTED\")\n",
        "            print(\"-\" * 70)\n",
        "            print(\"Best for: Forecasting last COMPLETE quarter\")\n",
        "            print(\"Run timing: ~7 days before BEA advance estimate\")\n",
        "            print(\"Data used: ALL data about last complete quarter\")\n",
        "            print(\"Accuracy: MAXIMUM (matches training data format)\")\n",
        "            print(\"\")\n",
        "            print(\"Example:\")\n",
        "            print(\"  Today: January 21, 2025\")\n",
        "            print(\"  Forecast: Q4 2024 GDP (quarter ended Dec 31)\")\n",
        "            print(\"  Data: Complete Q4 through all lagged releases\")\n",
        "            print(\"  BEA: Releases Jan 28 (you're 7 days ahead)\")\n",
        "            print(\"\")\n",
        "            print(\"✓ All 16 time-series complete\")\n",
        "            print(\"✓ Full 90-day quarterly averages\")\n",
        "            print(\"✓ Matches model training format\")\n",
        "\n",
        "    def show_dynamic_info(b: widgets.Button) -> None:\n",
        "        \"\"\"Display dynamic mode information.\n",
        "\n",
        "        Args:\n",
        "            b: Button widget that triggered event.\n",
        "        \"\"\"\n",
        "        global forecast_mode\n",
        "        forecast_mode = 'dynamic'\n",
        "\n",
        "        # Update button styles.\n",
        "        btn_static.button_style = ''\n",
        "        btn_dynamic.button_style = 'info'\n",
        "\n",
        "        with info_output:\n",
        "            clear_output()\n",
        "            print(\"DYNAMIC MODE SELECTED\")\n",
        "            print(\"-\" * 70)\n",
        "            print(\"Best for: Real-time 'nowcast' of CURRENT quarter\")\n",
        "            print(\"Run timing: Anytime (weekly, daily)\")\n",
        "            print(\"Data used: Includes PARTIAL current quarter data\")\n",
        "            print(\"Accuracy: LOWER (incomplete data, format mismatch)\")\n",
        "            print(\"\")\n",
        "            print(\"Example:\")\n",
        "            print(\"  Today: January 21, 2025 (mid-Q1)\")\n",
        "            print(\"  Forecast: Q1 2025 GDP (quarter ends Mar 31)\")\n",
        "            print(\"  Data: Only 21 days of Q1 available so far\")\n",
        "            print(\"  Warning: Using 21-day avg instead of 90-day avg\")\n",
        "            print(\"\")\n",
        "            print(\"⚠ Some time-series may be incomplete\")\n",
        "            print(\"⚠ Partial quarter ≠ training data format\")\n",
        "            print(\"⚠ Lower accuracy, use for trend monitoring only\")\n",
        "            print(\"\")\n",
        "            print(\"CAVEAT: Dynamic mode trades accuracy for timeliness\")\n",
        "\n",
        "    # Attach click handlers.\n",
        "    btn_static.on_click(show_static_info)\n",
        "    btn_dynamic.on_click(show_dynamic_info)\n",
        "\n",
        "    # Initialize with static mode selected.\n",
        "    show_static_info(None)\n",
        "\n",
        "    # Display interface.\n",
        "    display(widgets.VBox([\n",
        "        widgets.HTML(\n",
        "            \"<h4>Step 0: Select Forecast Mode</h4>\"\n",
        "            \"<p>Choose between maximum accuracy (static) or \"\n",
        "            \"real-time nowcast (dynamic):</p>\"\n",
        "        ),\n",
        "        widgets.HBox([btn_static, btn_dynamic]),\n",
        "        widgets.HTML(\"<hr>\"),\n",
        "        info_output\n",
        "    ]))\n",
        "\n",
        "\n",
        "def create_gti_uploader() -> None:\n",
        "    \"\"\"Create widget for uploading GTI file from Notebook 1.\"\"\"\n",
        "    global gti_data\n",
        "\n",
        "    # Create upload widget with enhanced styling.\n",
        "    upload_widget = widgets.FileUpload(\n",
        "        accept='.xlsx',\n",
        "        multiple=False,\n",
        "        description='Select File:',\n",
        "        button_style='primary',  # Blue button style.\n",
        "        layout=widgets.Layout(width='400px')\n",
        "    )\n",
        "\n",
        "    output = widgets.Output()\n",
        "\n",
        "    def on_upload(change: Dict[str, Any]) -> None:\n",
        "        \"\"\"Handle GTI file upload.\n",
        "\n",
        "        Args:\n",
        "            change: Widget change event.\n",
        "        \"\"\"\n",
        "        global gti_data\n",
        "\n",
        "        with output:\n",
        "            clear_output()\n",
        "\n",
        "            if not upload_widget.value:\n",
        "                print(\"No file uploaded.\")\n",
        "                return\n",
        "\n",
        "            try:\n",
        "                # Extract uploaded file content and metadata.\n",
        "                uploaded_file = list(upload_widget.value.values())[0]\n",
        "                file_content = uploaded_file['content']\n",
        "                file_name = uploaded_file['metadata']['name']\n",
        "\n",
        "                # Read GTI Excel file from memory.\n",
        "                gti_df = pd.read_excel(io.BytesIO(file_content))\n",
        "\n",
        "                # Display file information.\n",
        "                print(f\"Loaded: {file_name}\")\n",
        "                print(f\"Shape: {gti_df.shape}\")\n",
        "                print(f\"Columns: {list(gti_df.columns)}\")\n",
        "\n",
        "                # Validate required GTI column exists.\n",
        "                # Accept either 'GTI' or 'GTI_Normalized_0_100'.\n",
        "                gti_col = None\n",
        "                if 'GTI_Normalized_0_100' in gti_df.columns:\n",
        "                    gti_col = 'GTI_Normalized_0_100'\n",
        "                elif 'GTI' in gti_df.columns:\n",
        "                    gti_col = 'GTI'\n",
        "                    # Rename for consistency.\n",
        "                    gti_df = gti_df.rename(columns={'GTI': 'GTI_Normalized_0_100'})\n",
        "\n",
        "                if gti_col is None:\n",
        "                    print(\n",
        "                        \"\\nError: File must contain either \"\n",
        "                        \"'GTI' or 'GTI_Normalized_0_100' column\"\n",
        "                    )\n",
        "                    gti_data = None\n",
        "                    return\n",
        "\n",
        "                # Convert date column to datetime if present.\n",
        "                if 'date' in gti_df.columns:\n",
        "                    gti_df['date'] = pd.to_datetime(gti_df['date'])\n",
        "                    gti_df = gti_df.set_index('date')\n",
        "\n",
        "                # Store GTI data globally.\n",
        "                gti_data = gti_df\n",
        "\n",
        "                # Display success message and preview.\n",
        "                print(f\"\\nSuccess: GTI data loaded\")\n",
        "                print(f\"Periods: {len(gti_data)}\")\n",
        "                print(f\"\\nFirst few rows:\")\n",
        "                print(gti_data.round(2).head())\n",
        "\n",
        "            except Exception as error:\n",
        "                print(f\"Error loading GTI file: {error}\")\n",
        "                gti_data = None\n",
        "\n",
        "    # Attach upload event handler.\n",
        "    upload_widget.observe(on_upload, names='value')\n",
        "\n",
        "    # Display upload interface.\n",
        "    display(widgets.VBox([\n",
        "        widgets.HTML(\n",
        "            \"<h4>Step 1: Upload GTI from Notebook 1</h4>\"\n",
        "            \"<p><b>Upload the GTI_Quarterly_PCA.xlsx file generated by \"\n",
        "            \"Notebook 1.</b></p>\"\n",
        "        ),\n",
        "        upload_widget,\n",
        "        output\n",
        "    ]))\n",
        "\n",
        "\n",
        "def create_fetch_button() -> None:\n",
        "    \"\"\"Create button to fetch economic data.\"\"\"\n",
        "    global economic_data\n",
        "\n",
        "    button = widgets.Button(\n",
        "        description='Fetch Economic Data',\n",
        "        button_style='primary',\n",
        "        icon='download',\n",
        "        layout=widgets.Layout(width='250px')\n",
        "    )\n",
        "\n",
        "    output = widgets.Output()\n",
        "\n",
        "    def on_click(b: widgets.Button) -> None:\n",
        "        \"\"\"Handle fetch button click.\n",
        "\n",
        "        Args:\n",
        "            b: Button widget.\n",
        "        \"\"\"\n",
        "        global economic_data\n",
        "\n",
        "        with output:\n",
        "            clear_output()\n",
        "\n",
        "            print(\"=\" * 70)\n",
        "            print(\"FETCHING ECONOMIC DATA\")\n",
        "            print(\"=\" * 70)\n",
        "            print(\"\")\n",
        "\n",
        "            economic_data = fetch_all_economic_indicators()\n",
        "\n",
        "            if economic_data is not None:\n",
        "                print(\"=\" * 70)\n",
        "                print(\"DATA FETCH COMPLETE\")\n",
        "                print(\"=\" * 70)\n",
        "                print(f\"\\nColumns: {list(economic_data.columns)}\")\n",
        "                print(f\"\\nSample data:\")\n",
        "                print(economic_data.round(2).head())\n",
        "                print(f\"\\nReady to merge with GTI.\")\n",
        "            else:\n",
        "                print(\"\\nError: Failed to fetch economic data.\")\n",
        "\n",
        "    button.on_click(on_click)\n",
        "\n",
        "    display(widgets.VBox([\n",
        "        widgets.HTML(\"<br><h4>Step 2: Fetch Economic Indicators</h4>\"),\n",
        "        button,\n",
        "        output\n",
        "    ]))\n",
        "\n",
        "\n",
        "def create_merge_button() -> None:\n",
        "    \"\"\"Create button to merge data and export.\"\"\"\n",
        "    button = widgets.Button(\n",
        "        description='Export: XLSX#1 or XLSX#2',\n",
        "        button_style='success',\n",
        "        icon='check',\n",
        "        layout=widgets.Layout(width='250px')\n",
        "    )\n",
        "\n",
        "    output = widgets.Output()\n",
        "\n",
        "    def on_click(b: widgets.Button) -> None:\n",
        "        \"\"\"Handle merge button click.\n",
        "\n",
        "        Args:\n",
        "            b: Button widget.\n",
        "        \"\"\"\n",
        "        global gti_data, economic_data, forecast_mode\n",
        "\n",
        "        with output:\n",
        "            clear_output()\n",
        "\n",
        "            print(\"=\" * 70)\n",
        "            print(\"MERGING DATA AND EXPORTING\")\n",
        "            print(\"=\" * 70)\n",
        "            print(\"\")\n",
        "\n",
        "            if gti_data is None:\n",
        "                print(\"Error: Please upload GTI file first (Step 1).\")\n",
        "                return\n",
        "\n",
        "            if economic_data is None:\n",
        "                print(\"Error: Please fetch economic data first (Step 2).\")\n",
        "                return\n",
        "\n",
        "            try:\n",
        "                print(\"Merging GTI with economic indicators...\")\n",
        "\n",
        "                # Merge GTI with economic data on date index.\n",
        "                merged = economic_data.copy()\n",
        "                merged['GTI_Normalized_0_100'] = gti_data['GTI_Normalized_0_100']\n",
        "\n",
        "                # MODE-SPECIFIC NaN HANDLING.\n",
        "\n",
        "                if forecast_mode == 'static':\n",
        "                    # STATIC MODE: Drop rows with ANY missing data.\n",
        "                    merged = merged.dropna()\n",
        "                    print(\"Static mode: Dropped incomplete rows\")\n",
        "                else:\n",
        "                    # DYNAMIC MODE: Keep rows even if GDP missing.\n",
        "                    # Only drop if GTI is missing (critical).\n",
        "                    merged = merged.dropna(subset=['GTI_Normalized_0_100'])\n",
        "                    print(\"Dynamic mode: Kept rows with missing GDP\")\n",
        "\n",
        "                # Display merge results.\n",
        "                print(f\"Merged dataset: {merged.shape}\")\n",
        "                if len(merged) > 0:\n",
        "                    print(f\"Date range: {merged.index[0].date()} to \"\n",
        "                          f\"{merged.index[-1].date()}\")\n",
        "\n",
        "                # Reset index to convert date from index to column.\n",
        "                # This makes date a regular column for Excel export.\n",
        "                merged = merged.reset_index()\n",
        "\n",
        "                # Rename index column to 'date'.\n",
        "                if 'index' in merged.columns:\n",
        "                    merged = merged.rename(columns={'index': 'date'})\n",
        "                elif merged.columns[0] != 'date':\n",
        "                    # First column is the date, rename it.\n",
        "                    merged = merged.rename(columns={merged.columns[0]: 'date'})\n",
        "\n",
        "                # Reorder columns to match expected format.\n",
        "                # This order matches user's original data structure.\n",
        "                column_order = [\n",
        "                    'date',                          # Quarter start date\n",
        "                    'yield_10yr',                    # 10-year Treasury yield\n",
        "                    'gas_price',                     # Regular gasoline price\n",
        "                    'jobless_claims_quarterly_avg',  # Initial claims\n",
        "                    'vix_quarterly_avg',             # Volatility index\n",
        "                    'CorpYield_QtrAvg',             # Corporate bond yield\n",
        "                    'UNRATE_QtrAvg',                # Unemployment rate\n",
        "                    'GTI_Normalized_0_100',         # Google Trends Index\n",
        "                    'gdp',                          # Gross Domestic Product\n",
        "                    'gdp_pct_change_target'         # Target variable\n",
        "                ]\n",
        "\n",
        "                # Reindex with specified column order.\n",
        "                merged = merged[column_order]\n",
        "\n",
        "                # Round all numeric columns to 2 decimal places.\n",
        "                numeric_cols = merged.select_dtypes(include=['float64', 'int64']).columns\n",
        "                merged[numeric_cols] = merged[numeric_cols].round(2)\n",
        "\n",
        "                # Export to Excel file with mode-specific filename.\n",
        "                if forecast_mode == 'static':\n",
        "                    output_filename = 'XLSX#1_Economic_Data_Static.xlsx'\n",
        "                else:\n",
        "                    output_filename = 'XLSX#2_Economic_Data_Dynamic.xlsx'\n",
        "\n",
        "                merged.to_excel(output_filename, index=False)\n",
        "\n",
        "                # Display export success message.\n",
        "                print(f\"\\nSuccess: Exported to {output_filename}\")\n",
        "                print(f\"Total rows: {len(merged)}\")\n",
        "                print(f\"Total columns: {len(merged.columns)}\")\n",
        "\n",
        "                # Display column summary for verification.\n",
        "                print(\"\\nColumn summary:\")\n",
        "                for i, col in enumerate(merged.columns, 1):\n",
        "                    print(f\"  {i:2d}. {col}\")\n",
        "\n",
        "                # Display first few rows as preview.\n",
        "                print(\"\\nFirst few rows:\")\n",
        "                print(merged.round(2).head())\n",
        "\n",
        "                print(\"\\n\" + \"=\" * 70)\n",
        "\n",
        "                # Display mode-specific completion messages.\n",
        "                if forecast_mode == 'static':\n",
        "                    # Static mode: Maximum accuracy messaging.\n",
        "                    print(\"STATIC MODE: DATASET READY FOR MAXIMUM ACCURACY\")\n",
        "                    print(\"=\" * 70)\n",
        "                    print(\"✓ All quarters complete (90-day averages)\")\n",
        "                    print(\"✓ Matches model training format\")\n",
        "                    print(\"✓ Ready for accurate GDP forecasting\")\n",
        "                    print(\"\")\n",
        "                    print(f\"Last quarter: {merged['date'].iloc[-1].date()}\")\n",
        "                    print(\"Run forecast ~7 days before BEA advance estimate\")\n",
        "                else:\n",
        "                    # Dynamic mode: Experimental nowcast messaging.\n",
        "                    print(\"DYNAMIC MODE: DATASET READY (EXPERIMENTAL)\")\n",
        "                    print(\"=\" * 70)\n",
        "                    print(\"⚠ May include partial current quarter\")\n",
        "                    print(\"⚠ Lower accuracy than static mode\")\n",
        "                    print(\"⚠ Use for trend monitoring, not precision\")\n",
        "                    print(\"\")\n",
        "                    print(f\"Last quarter: {merged['date'].iloc[-1].date()}\")\n",
        "                    print(\"Consider switching to STATIC mode for BEA comparison\")\n",
        "\n",
        "                print(\"\")\n",
        "                print(f\"Proceed to Cell 0 to upload this file.\")\n",
        "\n",
        "                # Attempt download in Google Colab environment.\n",
        "                try:\n",
        "                    from google.colab import files\n",
        "                    files.download(output_filename)\n",
        "                    print(f\"\\nDownload initiated: {output_filename}\")\n",
        "                except Exception:\n",
        "                    # Not in Colab or download failed.\n",
        "                    # File still saved in filesystem.\n",
        "                    print(f\"\\nFile saved: {output_filename}\")\n",
        "\n",
        "            except Exception as error:\n",
        "                print(f\"Error during merge: {error}\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "\n",
        "    button.on_click(on_click)\n",
        "\n",
        "    display(widgets.VBox([\n",
        "        widgets.HTML(\"<br><h4>Step 3: Merge Data and Export</h4>\"),\n",
        "        button,\n",
        "        output\n",
        "    ]))\n",
        "\n",
        "\n",
        "# Install dependencies and display interface.\n",
        "install_dependencies()\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"AUTOMATED ECONOMIC DATA FETCHER\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\")\n",
        "print(\"This cell automates the multi-day data scrubbing process:\")\n",
        "print(\"  1. Fetch indicators from FRED and Yahoo Finance\")\n",
        "print(\"  2. Resample daily/weekly/monthly to quarterly\")\n",
        "print(\"  3. Align all series to common dates\")\n",
        "print(\"  4. Merge with GTI from Notebook 1\")\n",
        "print(\"  5. Export: 5TS_A_NO_LEAK.xlsx\")\n",
        "print(\"\")\n",
        "print(\"TWO FORECAST MODES AVAILABLE:\")\n",
        "print(\"-\" * 70)\n",
        "print(\"STATIC (Recommended): Maximum accuracy, ~7-day BEA head start\")\n",
        "print(\"DYNAMIC (Experimental): Anytime nowcast, lower accuracy\")\n",
        "print(\"\")\n",
        "print(\"Follow the 4 steps below:\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "create_mode_selector()\n",
        "create_gti_uploader()\n",
        "create_fetch_button()\n",
        "create_merge_button()\n",
        "\n",
        "print(\"\")\n",
        "print(\"Note: Requires internet connection.\")\n",
        "print(\"Data sources: BEA (GDP), FRED (Federal Reserve), and Yahoo Finance.\")"
      ],
      "metadata": {
        "id": "rPUCfFuGImq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# RAW DATA ANALYSIS - CODE BLOCK 2\n",
        "# =============================================================================\n",
        "\"\"\"Analyze raw data from code block 1 output xlsx file.\"\"\"\n",
        "\n",
        "from typing import Optional, Dict, Any\n",
        "import io\n",
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "# Try to access df from previous run, otherwise initialize to None\n",
        "try:\n",
        "    if df is not None:\n",
        "        pass  # df already exists from previous upload\n",
        "except NameError:\n",
        "    df = None  # First time running this cell\n",
        "\n",
        "\n",
        "def create_data_uploader() -> None:\n",
        "    \"\"\"Create widget for uploading xlsx from code block 1.\"\"\"\n",
        "    global df\n",
        "\n",
        "    upload_widget = widgets.FileUpload(\n",
        "        accept='.xlsx',\n",
        "        multiple=False,\n",
        "        description='Select File:',\n",
        "        button_style='primary',\n",
        "        layout=widgets.Layout(width='400px')\n",
        "    )\n",
        "\n",
        "    output = widgets.Output()\n",
        "\n",
        "    def on_upload(change: Dict[str, Any]) -> None:\n",
        "        \"\"\"Handle file upload.\n",
        "\n",
        "        Args:\n",
        "            change: Widget change event.\n",
        "        \"\"\"\n",
        "        global df\n",
        "\n",
        "        with output:\n",
        "            clear_output()\n",
        "\n",
        "            if not upload_widget.value:\n",
        "                print('No file uploaded.')\n",
        "                return\n",
        "\n",
        "            try:\n",
        "                # Extract uploaded file content\n",
        "                uploaded_file = list(upload_widget.value.values())[0]\n",
        "                file_content = uploaded_file['content']\n",
        "                file_name = uploaded_file['metadata']['name']\n",
        "\n",
        "                # Read Excel file\n",
        "                df = pd.read_excel(io.BytesIO(file_content))\n",
        "\n",
        "                print(f'Loaded: {file_name}')\n",
        "                print(f'Shape: {df.shape}')\n",
        "                print(f'Columns: {list(df.columns)}')\n",
        "\n",
        "                print('\\nSuccess: Data loaded')\n",
        "                print('\\nRe-run this cell to see visualizations')\n",
        "\n",
        "            except Exception as error:\n",
        "                print(f'Error loading file: {error}')\n",
        "                df = None\n",
        "\n",
        "    upload_widget.observe(on_upload, names='value')\n",
        "\n",
        "    display(widgets.VBox([\n",
        "        widgets.HTML(\n",
        "            '<h4>Upload Code Block 1 Output</h4>'\n",
        "            '<p>Upload the xlsx file downloaded from code block 1:</p>'\n",
        "        ),\n",
        "        upload_widget,\n",
        "        output\n",
        "    ]))\n",
        "\n",
        "\n",
        "def analyze_correlations() -> Optional[bool]:\n",
        "    \"\"\"Analyze feature correlations with GDP target.\n",
        "\n",
        "    Returns:\n",
        "        True if successful, None if error.\n",
        "    \"\"\"\n",
        "    global df\n",
        "\n",
        "    if df is None:\n",
        "        return None\n",
        "\n",
        "    print('=' * 80)\n",
        "    print('FEATURE CORRELATION ANALYSIS')\n",
        "    print('=' * 80)\n",
        "\n",
        "    # Seven input features\n",
        "    features = [\n",
        "        'yield_10yr', 'gas_price', 'jobless_claims_quarterly_avg',\n",
        "        'vix_quarterly_avg', 'CorpYield_QtrAvg', 'UNRATE_QtrAvg',\n",
        "        'GTI_Normalized_0_100'\n",
        "    ]\n",
        "\n",
        "    # Calculate correlations\n",
        "    corrs = {}\n",
        "    for feat in features:\n",
        "        if feat in df.columns:\n",
        "            valid = df[[feat, 'gdp_pct_change_target']].dropna()\n",
        "            if len(valid) > 0:\n",
        "                corrs[feat] = abs(\n",
        "                    valid[feat].corr(valid['gdp_pct_change_target'])\n",
        "                )\n",
        "\n",
        "    # Sort by strength\n",
        "    sorted_corrs = sorted(corrs.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Display table\n",
        "    print(f\"\\n{'Rank':<6}{'Feature':<35}{'Correlation':<15}\")\n",
        "    print('-' * 56)\n",
        "    for i, (feat, corr) in enumerate(sorted_corrs, 1):\n",
        "        print(f'{i:<6}{feat:<35}{corr:>14.4f}')\n",
        "\n",
        "    # Visualize\n",
        "    feats, vals = zip(*sorted_corrs)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "    bars = ax.barh(\n",
        "        range(len(feats)), vals,\n",
        "        color='steelblue', alpha=0.8, edgecolor='black'\n",
        "    )\n",
        "\n",
        "    ax.set_yticks(range(len(feats)))\n",
        "    ax.set_yticklabels(feats, fontsize=11)\n",
        "    ax.set_xlabel('Absolute Correlation', fontsize=12)\n",
        "    ax.set_title(\n",
        "        'Feature Correlations with GDP Growth',\n",
        "        fontsize=14, fontweight='bold'\n",
        "    )\n",
        "    ax.grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "    # Add labels\n",
        "    for bar, corr in zip(bars, vals):\n",
        "        ax.text(\n",
        "            bar.get_width() + 0.01,\n",
        "            bar.get_y() + bar.get_height() / 2.,\n",
        "            f'{corr:.4f}',\n",
        "            ha='left', va='center', fontsize=10, fontweight='bold'\n",
        "        )\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "def visualize_gdp_timeseries() -> Optional[bool]:\n",
        "    \"\"\"Create GDP time series visualization.\n",
        "\n",
        "    Returns:\n",
        "        True if successful, None if error.\n",
        "    \"\"\"\n",
        "    global df\n",
        "\n",
        "    if df is None:\n",
        "        return None\n",
        "\n",
        "    print('\\n' + '=' * 80)\n",
        "    print('GDP GROWTH TIME SERIES')\n",
        "    print('=' * 80)\n",
        "\n",
        "    # Extract data\n",
        "    data = df[['date', 'gdp_pct_change_target']].dropna()\n",
        "    data['date'] = pd.to_datetime(data['date'])\n",
        "\n",
        "    # Create plot\n",
        "    fig, ax = plt.subplots(figsize=(16, 8))\n",
        "\n",
        "    ax.plot(\n",
        "        data['date'], data['gdp_pct_change_target'],\n",
        "        linewidth=2, color='darkblue', label='GDP Growth Rate'\n",
        "    )\n",
        "\n",
        "    # Highlight recessions\n",
        "    recession = data['gdp_pct_change_target'] < 0\n",
        "    ax.fill_between(\n",
        "        data['date'], data['gdp_pct_change_target'], 0,\n",
        "        where=recession, color='red', alpha=0.3, label='Recession'\n",
        "    )\n",
        "\n",
        "    # Highlight growth\n",
        "    growth = data['gdp_pct_change_target'] > 0\n",
        "    ax.fill_between(\n",
        "        data['date'], 0, data['gdp_pct_change_target'],\n",
        "        where=growth, color='green', alpha=0.3, label='Growth'\n",
        "    )\n",
        "\n",
        "    ax.axhline(0, color='black', linewidth=1, alpha=0.5)\n",
        "    ax.set_xlabel('Date', fontsize=12)\n",
        "    ax.set_ylabel('GDP Growth Rate (%)', fontsize=12)\n",
        "    ax.set_title(\n",
        "        'GDP Growth Over Time',\n",
        "        fontsize=14, fontweight='bold'\n",
        "    )\n",
        "    ax.legend(fontsize=10)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Statistics\n",
        "    print(f\"\\nQuarters: {len(data)}\")\n",
        "    print(f'Mean: {data[\"gdp_pct_change_target\"].mean():.2f}%')\n",
        "    print(f'Std: {data[\"gdp_pct_change_target\"].std():.2f}%')\n",
        "    print(f'Min: {data[\"gdp_pct_change_target\"].min():.2f}%')\n",
        "    print(f'Max: {data[\"gdp_pct_change_target\"].max():.2f}%')\n",
        "\n",
        "    rec_count = (data['gdp_pct_change_target'] < 0).sum()\n",
        "    print(f'\\nRecession quarters: {rec_count}')\n",
        "    print(f'Recession %: {rec_count / len(data) * 100:.1f}%')\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "# Main execution\n",
        "if df is None:\n",
        "    # Show upload widget\n",
        "    create_data_uploader()\n",
        "else:\n",
        "    # Run analysis\n",
        "    print('=' * 80)\n",
        "    print('RAW DATA ANALYSIS')\n",
        "    print('=' * 80)\n",
        "    print()\n",
        "\n",
        "    analyze_correlations()\n",
        "    visualize_gdp_timeseries()\n",
        "\n",
        "    print('\\n' + '=' * 80)\n",
        "    print('ANALYSIS COMPLETE')\n",
        "    print('=' * 80)"
      ],
      "metadata": {
        "id": "9BwAO34gd6We"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# ENGINEERED FEATURES GENERATOR WITH EXCEL EXPORT\n",
        "# =============================================================================\n",
        "\"\"\"Generate engineered features for GDP forecasting.\n",
        "\n",
        "This module creates 5 engineered features from the 7 raw input features:\n",
        "1. gdp_volatility: Rolling standard deviation of GDP growth\n",
        "2. term_spread: 10-year yield minus 2-year approximation\n",
        "3. credit_spread: Corporate yield minus Treasury yield\n",
        "4. labor_stress: Jobless claims multiplied by unemployment rate\n",
        "5. financial_stress: Composite index of market stress\n",
        "\n",
        "All features use leakage-proof methodology with proper temporal handling.\n",
        "Exports to Excel with formula documentation and highlighting.\n",
        "Auto-downloads to user's local machine.\n",
        "\"\"\"\n",
        "\n",
        "from typing import Optional, Tuple, Dict, Any\n",
        "import io\n",
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from openpyxl import Workbook, load_workbook\n",
        "from openpyxl.comments import Comment\n",
        "from openpyxl.styles import Font, PatternFill, Alignment\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output, HTML\n",
        "import base64\n",
        "import os\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "# Check if df exists from previous run BEFORE defining it\n",
        "try:\n",
        "    if df is not None:\n",
        "        pass  # df already exists from previous upload\n",
        "except NameError:\n",
        "    df = None  # First time running this cell\n",
        "\n",
        "\n",
        "def create_data_uploader() -> None:\n",
        "    \"\"\"Create widget for uploading xlsx from code block 2.\n",
        "\n",
        "    Allows user to upload the Excel file output from code block 2.\n",
        "    Sets global df variable when file is uploaded successfully.\n",
        "    \"\"\"\n",
        "    global df\n",
        "\n",
        "    upload_widget = widgets.FileUpload(\n",
        "        accept='.xlsx',\n",
        "        multiple=False,\n",
        "        description='Select File:',\n",
        "        button_style='primary',\n",
        "        layout=widgets.Layout(width='400px')\n",
        "    )\n",
        "\n",
        "    output = widgets.Output()\n",
        "\n",
        "    def on_upload(change: Dict[str, Any]) -> None:\n",
        "        \"\"\"Handle file upload.\n",
        "\n",
        "        Args:\n",
        "            change: Widget change event dictionary.\n",
        "        \"\"\"\n",
        "        global df\n",
        "\n",
        "        with output:\n",
        "            clear_output()\n",
        "\n",
        "            if not upload_widget.value:\n",
        "                print('No file uploaded.')\n",
        "                return\n",
        "\n",
        "            try:\n",
        "                # Extract uploaded file content\n",
        "                uploaded_file = list(upload_widget.value.values())[0]\n",
        "                file_content = uploaded_file['content']\n",
        "                file_name = uploaded_file['metadata']['name']\n",
        "\n",
        "                # Read Excel file from memory\n",
        "                df = pd.read_excel(io.BytesIO(file_content))\n",
        "\n",
        "                print(f'Loaded: {file_name}')\n",
        "                print(f'Shape: {df.shape}')\n",
        "                print(f'Columns: {list(df.columns)}')\n",
        "                print('\\nSuccess: Data loaded')\n",
        "                print('Re-run this cell to generate engineered features')\n",
        "\n",
        "            except Exception as error:\n",
        "                print(f'Error loading file: {error}')\n",
        "                df = None\n",
        "\n",
        "    upload_widget.observe(on_upload, names='value')\n",
        "\n",
        "    display(widgets.VBox([\n",
        "        widgets.HTML(\n",
        "            '<h4>Upload Code Block 2 Output</h4>'\n",
        "            '<p>Upload the xlsx file from code block 2:</p>'\n",
        "        ),\n",
        "        upload_widget,\n",
        "        output\n",
        "    ]))\n",
        "\n",
        "\n",
        "def generate_engineered_features() -> Tuple[\n",
        "    Optional[pd.DataFrame], Optional[str]\n",
        "]:\n",
        "    \"\"\"Generate all engineered features with documentation.\n",
        "\n",
        "    Creates 5 engineered features from raw inputs:\n",
        "    - gdp_volatility: 4-quarter rolling std of GDP growth (shifted)\n",
        "    - term_spread: 10yr yield - 2yr approximation\n",
        "    - credit_spread: Corporate yield - Treasury yield\n",
        "    - labor_stress: jobless_claims * unemployment / 100\n",
        "    - financial_stress: Composite stress indicator\n",
        "\n",
        "    Exports results to Excel with comments and auto-downloads.\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (engineered_dataframe, output_filename) if successful,\n",
        "        (None, None) if error occurred.\n",
        "    \"\"\"\n",
        "    global df\n",
        "\n",
        "    if df is None:\n",
        "        print('Error: No data loaded.')\n",
        "        return None, None\n",
        "\n",
        "    print('=' * 80)\n",
        "    print('GENERATING ENGINEERED FEATURES FOR GDP FORECASTING')\n",
        "    print('=' * 80)\n",
        "\n",
        "    try:\n",
        "        # Create working copy of dataframe\n",
        "        df_engineered = df.copy()\n",
        "\n",
        "        # Display base data columns\n",
        "        print('\\nBASE DATA COLUMNS (B-H):')\n",
        "        base_columns = [\n",
        "            'yield_10yr', 'gas_price', 'jobless_claims_quarterly_avg',\n",
        "            'vix_quarterly_avg', 'CorpYield_QtrAvg', 'UNRATE_QtrAvg',\n",
        "            'GTI_Normalized_0_100'\n",
        "        ]\n",
        "\n",
        "        for i, col in enumerate(base_columns, 2):  # Start from B=2\n",
        "            if col in df.columns:\n",
        "                # Convert column index to letter (A=65 in ASCII)\n",
        "                col_letter = chr(64 + i)\n",
        "                print(f'   Column {col_letter}: {col}')\n",
        "\n",
        "        print('\\nCREATING ENGINEERED FEATURES:')\n",
        "\n",
        "        # FEATURE 1: GDP VOLATILITY\n",
        "        # Measures economic growth instability using rolling std\n",
        "        print('\\n1. GDP_VOLATILITY')\n",
        "        print('   Purpose: Measures GDP growth instability')\n",
        "        print('   Formula: Rolling 4-quarter standard deviation')\n",
        "        print('   Logic: Higher volatility precedes recessions')\n",
        "\n",
        "        if 'gdp_pct_change_target' in df_engineered.columns:\n",
        "            # Apply shift(1) before rolling to prevent target leakage\n",
        "            # This ensures we use only past GDP values\n",
        "            df_engineered['gdp_volatility'] = (\n",
        "                df_engineered['gdp_pct_change_target']\n",
        "                .shift(1)\n",
        "                .rolling(window=4, min_periods=2)\n",
        "                .std()\n",
        "            )\n",
        "\n",
        "            vol_min = df_engineered['gdp_volatility'].min()\n",
        "            vol_max = df_engineered['gdp_volatility'].max()\n",
        "            vol_avg = df_engineered['gdp_volatility'].mean()\n",
        "\n",
        "            print(f'   Created: Range {vol_min:.2f} to {vol_max:.2f}')\n",
        "            print(f'   Average: {vol_avg:.2f}')\n",
        "            print('   LEAKAGE-PROOF: Uses shift(1) before rolling')\n",
        "        else:\n",
        "            df_engineered['gdp_volatility'] = 0\n",
        "            print('   Warning: GDP target not available, set to 0')\n",
        "\n",
        "        # FEATURE 2: TERM SPREAD\n",
        "        # Yield curve slope indicator\n",
        "        print('\\n2. TERM_SPREAD')\n",
        "        print('   Purpose: Yield curve slope indicator')\n",
        "        print('   Formula: 10-Year Treasury Yield - 2.0%')\n",
        "        print('   Logic: Negative spread predicts recession')\n",
        "\n",
        "        if 'yield_10yr' in df_engineered.columns:\n",
        "            # Subtract typical 2-year yield approximation\n",
        "            df_engineered['term_spread'] = (\n",
        "                df_engineered['yield_10yr'] - 2.0\n",
        "            )\n",
        "\n",
        "            spread_min = df_engineered['term_spread'].min()\n",
        "            spread_max = df_engineered['term_spread'].max()\n",
        "            spread_avg = df_engineered['term_spread'].mean()\n",
        "\n",
        "            print(f'   Created: Range {spread_min:.2f} to '\n",
        "                  f'{spread_max:.2f}')\n",
        "            print(f'   Average: {spread_avg:.2f}')\n",
        "            print('   Negative values indicate inverted yield curve')\n",
        "        else:\n",
        "            df_engineered['term_spread'] = 0\n",
        "            print('   Warning: 10-year yield not available, set to 0')\n",
        "\n",
        "        # FEATURE 3: CREDIT SPREAD\n",
        "        # Corporate credit risk premium\n",
        "        print('\\n3. CREDIT_SPREAD')\n",
        "        print('   Purpose: Corporate credit risk premium')\n",
        "        print('   Formula: Corporate Yield - Treasury Yield')\n",
        "        print('   Logic: Widening spreads indicate credit stress')\n",
        "\n",
        "        if ('CorpYield_QtrAvg' in df_engineered.columns and\n",
        "                'yield_10yr' in df_engineered.columns):\n",
        "            # Calculate difference between corporate and risk-free rates\n",
        "            df_engineered['credit_spread'] = (\n",
        "                df_engineered['CorpYield_QtrAvg'] -\n",
        "                df_engineered['yield_10yr']\n",
        "            )\n",
        "\n",
        "            credit_min = df_engineered['credit_spread'].min()\n",
        "            credit_max = df_engineered['credit_spread'].max()\n",
        "            credit_avg = df_engineered['credit_spread'].mean()\n",
        "\n",
        "            print(f'   Created: Range {credit_min:.2f} to '\n",
        "                  f'{credit_max:.2f}')\n",
        "            print(f'   Average: {credit_avg:.2f}')\n",
        "            print('   Higher values indicate increased borrowing costs')\n",
        "        else:\n",
        "            # Use typical corporate spread as default\n",
        "            df_engineered['credit_spread'] = 1.5\n",
        "            print('   Warning: Yields not available, set to typical 1.5%')\n",
        "\n",
        "        # FEATURE 4: LABOR STRESS\n",
        "        # Amplifies labor market distress signals\n",
        "        print('\\n4. LABOR_STRESS')\n",
        "        print('   Purpose: Labor market stress amplifier')\n",
        "        print('   Formula: Jobless Claims * Unemployment Rate / 100')\n",
        "        print('   Logic: Combines weekly claims with overall rate')\n",
        "\n",
        "        if ('jobless_claims_quarterly_avg' in df_engineered.columns and\n",
        "                'UNRATE_QtrAvg' in df_engineered.columns):\n",
        "            # Multiply claims by unemployment rate percentage\n",
        "            df_engineered['labor_stress'] = (\n",
        "                df_engineered['jobless_claims_quarterly_avg'] *\n",
        "                df_engineered['UNRATE_QtrAvg'] / 100\n",
        "            )\n",
        "\n",
        "            labor_min = df_engineered['labor_stress'].min()\n",
        "            labor_max = df_engineered['labor_stress'].max()\n",
        "            labor_avg = df_engineered['labor_stress'].mean()\n",
        "\n",
        "            print(f'   Created: Range {labor_min:.2f} to '\n",
        "                  f'{labor_max:.2f}')\n",
        "            print(f'   Average: {labor_avg:.2f}')\n",
        "            print('   Higher values indicate severe labor distress')\n",
        "        else:\n",
        "            df_engineered['labor_stress'] = 0\n",
        "            print('   Warning: Labor data not available, set to 0')\n",
        "\n",
        "        # FEATURE 5: FINANCIAL STRESS\n",
        "        # Composite financial market stress indicator\n",
        "        print('\\n5. FINANCIAL_STRESS')\n",
        "        print('   Purpose: Composite financial stress indicator')\n",
        "        print('   Formula: ((VIX-20)/20) + (Credit*2) - Term')\n",
        "        print('   Logic: Combines volatility, credit, yield signals')\n",
        "\n",
        "        if 'vix_quarterly_avg' in df_engineered.columns:\n",
        "            # Normalize VIX around typical level of 20\n",
        "            # Weight credit spread heavily (2x)\n",
        "            # Invert term spread (negative is bad)\n",
        "            df_engineered['financial_stress'] = (\n",
        "                (df_engineered['vix_quarterly_avg'] - 20) / 20 +\n",
        "                df_engineered['credit_spread'] * 2 -\n",
        "                df_engineered['term_spread']\n",
        "            )\n",
        "\n",
        "            stress_min = df_engineered['financial_stress'].min()\n",
        "            stress_max = df_engineered['financial_stress'].max()\n",
        "            stress_avg = df_engineered['financial_stress'].mean()\n",
        "\n",
        "            print(f'   Created: Range {stress_min:.2f} to '\n",
        "                  f'{stress_max:.2f}')\n",
        "            print(f'   Average: {stress_avg:.2f}')\n",
        "            print('   Higher values indicate elevated financial stress')\n",
        "        else:\n",
        "            df_engineered['financial_stress'] = 0\n",
        "            print('   Warning: VIX not available, set to 0')\n",
        "\n",
        "        # SAVE TO EXCEL\n",
        "        output_filename = 'XLSX#3_Engineered_Features.xlsx'\n",
        "\n",
        "        # Define engineered features list\n",
        "        engineered_features = [\n",
        "            'gdp_volatility', 'term_spread', 'credit_spread',\n",
        "            'labor_stress', 'financial_stress'\n",
        "        ]\n",
        "\n",
        "        # Define column order for output (NO out_of_fold_forecast)\n",
        "        output_columns = (\n",
        "            ['date'] + base_columns +\n",
        "            ['gdp', 'gdp_pct_change_target'] +\n",
        "            engineered_features\n",
        "        )\n",
        "\n",
        "        # Filter to columns that exist\n",
        "        output_columns = [\n",
        "            col for col in output_columns\n",
        "            if col in df_engineered.columns\n",
        "        ]\n",
        "\n",
        "        # Create output dataframe with selected columns\n",
        "        output_df = df_engineered[output_columns].copy()\n",
        "\n",
        "        # Round numeric columns for readability\n",
        "        # Base features: 4 decimals\n",
        "        # Engineered features: 2 decimals\n",
        "        for col in output_df.columns:\n",
        "            if output_df[col].dtype in ['float64', 'float32']:\n",
        "                if col in engineered_features:\n",
        "                    # Round engineered features to 2 decimals\n",
        "                    output_df[col] = output_df[col].round(2)\n",
        "                else:\n",
        "                    # Round base features to 4 decimals\n",
        "                    output_df[col] = output_df[col].round(4)\n",
        "\n",
        "        # Save to Excel\n",
        "        output_df.to_excel(output_filename, index=False)\n",
        "\n",
        "        # Add comments and formatting using openpyxl\n",
        "        wb = load_workbook(output_filename)\n",
        "        ws = wb.active\n",
        "\n",
        "        # Define feature explanations for Excel comments\n",
        "        feature_explanations = {\n",
        "            'gdp_volatility': (\n",
        "                'GDP VOLATILITY (LEAKAGE-PROOF)\\n\\n'\n",
        "                'Formula: shift(1).rolling(4).std()\\n'\n",
        "                'Purpose: Measures GDP growth instability\\n'\n",
        "                'Interpretation: Higher = more volatile economy'\n",
        "            ),\n",
        "            'term_spread': (\n",
        "                'TERM SPREAD\\n\\n'\n",
        "                'Formula: 10-Year Yield - 2.0%\\n'\n",
        "                'Purpose: Yield curve slope indicator\\n'\n",
        "                'Interpretation: Negative = inverted curve = '\n",
        "                'recession signal'\n",
        "            ),\n",
        "            'credit_spread': (\n",
        "                'CREDIT SPREAD\\n\\n'\n",
        "                'Formula: Corporate Yield - Treasury Yield\\n'\n",
        "                'Purpose: Corporate credit risk premium\\n'\n",
        "                'Interpretation: Widening = increased credit stress'\n",
        "            ),\n",
        "            'labor_stress': (\n",
        "                'LABOR STRESS INDICATOR\\n\\n'\n",
        "                'Formula: Jobless Claims * Unemployment / 100\\n'\n",
        "                'Purpose: Amplifies labor market distress\\n'\n",
        "                'Interpretation: Higher = severe labor stress'\n",
        "            ),\n",
        "            'financial_stress': (\n",
        "                'FINANCIAL STRESS INDEX\\n\\n'\n",
        "                'Formula: ((VIX-20)/20) + (Credit*2) - Term\\n'\n",
        "                'Purpose: Composite market stress indicator\\n'\n",
        "                'Interpretation: Higher = elevated stress'\n",
        "            )\n",
        "        }\n",
        "\n",
        "        # Add comments and highlighting to header row\n",
        "        for col_idx, col_name in enumerate(output_df.columns, 1):\n",
        "            cell = ws.cell(row=1, column=col_idx)\n",
        "\n",
        "            if col_name in feature_explanations:\n",
        "                # Add comment with explanation\n",
        "                comment = Comment(\n",
        "                    feature_explanations[col_name],\n",
        "                    'GDP_Model_System'\n",
        "                )\n",
        "                comment.width = 400\n",
        "                comment.height = 200\n",
        "                cell.comment = comment\n",
        "\n",
        "                # Yellow highlighting for engineered features\n",
        "                cell.fill = PatternFill(\n",
        "                    start_color='FFFF99',\n",
        "                    end_color='FFFF99',\n",
        "                    fill_type='solid'\n",
        "                )\n",
        "                cell.font = Font(bold=True)\n",
        "\n",
        "        # Create documentation sheet\n",
        "        info_sheet = wb.create_sheet('Feature_Documentation')\n",
        "\n",
        "        # Add header\n",
        "        info_sheet['A1'] = (\n",
        "            'GDP FORECASTING MODEL - '\n",
        "            'LEAKAGE-PROOF ENGINEERED FEATURES'\n",
        "        )\n",
        "        info_sheet['A1'].font = Font(bold=True, size=14)\n",
        "\n",
        "        row = 3\n",
        "\n",
        "        # Leakage prevention section\n",
        "        info_sheet[f'A{row}'] = 'LEAKAGE PREVENTION MEASURES:'\n",
        "        info_sheet[f'A{row}'].font = Font(bold=True)\n",
        "        row += 1\n",
        "\n",
        "        info_sheet[f'A{row}'] = (\n",
        "            '- GDP volatility uses shift(1) before rolling'\n",
        "        )\n",
        "        row += 1\n",
        "        info_sheet[f'A{row}'] = (\n",
        "            '- All temporal safeguards implemented'\n",
        "        )\n",
        "        row += 1\n",
        "        info_sheet[f'A{row}'] = (\n",
        "            '- Forbidden features excluded from calculations'\n",
        "        )\n",
        "        row += 2\n",
        "\n",
        "        # Base columns section\n",
        "        info_sheet[f'A{row}'] = 'BASE DATA COLUMNS (B-H):'\n",
        "        info_sheet[f'A{row}'].font = Font(bold=True)\n",
        "        row += 1\n",
        "\n",
        "        for col in base_columns:\n",
        "            if col in df.columns:\n",
        "                info_sheet[f'A{row}'] = f'- {col}'\n",
        "                row += 1\n",
        "\n",
        "        row += 1\n",
        "\n",
        "        # Engineered features section\n",
        "        info_sheet[f'A{row}'] = 'ENGINEERED FEATURES:'\n",
        "        info_sheet[f'A{row}'].font = Font(bold=True)\n",
        "        row += 1\n",
        "\n",
        "        for feature, explanation in feature_explanations.items():\n",
        "            info_sheet[f'A{row}'] = f'- {feature.upper()}'\n",
        "            info_sheet[f'A{row}'].font = Font(bold=True)\n",
        "            row += 1\n",
        "\n",
        "            # Add explanation lines\n",
        "            for line in explanation.split('\\n'):\n",
        "                if line.strip():\n",
        "                    info_sheet[f'A{row}'] = f'  {line}'\n",
        "                    row += 1\n",
        "            row += 1\n",
        "\n",
        "        # Save Excel file with formatting\n",
        "        wb.save(output_filename)\n",
        "\n",
        "        print(f'\\nEXCEL FILE CREATED: {output_filename}')\n",
        "        print(f'   Data Sheet: {len(output_df)} rows x '\n",
        "              f'{len(output_df.columns)} columns')\n",
        "        print('   Documentation Sheet: Feature explanations')\n",
        "        print('   Comments: Hover over headers for details')\n",
        "        print('   Highlighting: Engineered features (yellow)')\n",
        "        print('   LEAKAGE-PROOF: All temporal safeguards implemented')\n",
        "\n",
        "        # AUTO-DOWNLOAD LOGIC\n",
        "        try:\n",
        "            if os.path.exists(output_filename):\n",
        "                file_size = os.path.getsize(output_filename) / 1024\n",
        "\n",
        "                with open(output_filename, 'rb') as f:\n",
        "                    file_data = f.read()\n",
        "\n",
        "                b64_data = base64.b64encode(file_data).decode()\n",
        "                download_link = (\n",
        "                    f'<a href=\"data:application/vnd.openxmlformats-'\n",
        "                    f'officedocument.spreadsheetml.sheet;base64,'\n",
        "                    f'{b64_data}\" download=\"{output_filename}\" '\n",
        "                    f'style=\"background-color: #4CAF50; '\n",
        "                    f'color: white; padding: 10px 20px; '\n",
        "                    f'text-decoration: none; border-radius: 5px; '\n",
        "                    f'font-weight: bold;\">DOWNLOAD '\n",
        "                    f'{output_filename} ({file_size:.1f} KB)</a>'\n",
        "                )\n",
        "\n",
        "                print('\\nAUTO-DOWNLOAD LINK:')\n",
        "                display(HTML(\n",
        "                    f'<div style=\"text-align: center; '\n",
        "                    f'margin: 20px;\">{download_link}</div>'\n",
        "                ))\n",
        "\n",
        "        except Exception as download_error:\n",
        "            print(f'\\nAuto-download setup failed: '\n",
        "                  f'{download_error}')\n",
        "            print(f'Manual download: Look for {output_filename} '\n",
        "                  f'in file browser')\n",
        "\n",
        "        # Display feature statistics\n",
        "        print('\\nENGINEERED FEATURES STATISTICS:')\n",
        "        print('-' * 60)\n",
        "\n",
        "        for feature in engineered_features:\n",
        "            if feature in df_engineered.columns:\n",
        "                values = df_engineered[feature].dropna()\n",
        "                if len(values) > 0:\n",
        "                    print(f'   {feature}:')\n",
        "                    print(f'     Range: {values.min():.2f} to '\n",
        "                          f'{values.max():.2f}')\n",
        "                    print(f'     Mean: {values.mean():.2f}, '\n",
        "                          f'Std: {values.std():.2f}')\n",
        "                    print(f'     Non-null: {len(values)}/'\n",
        "                          f'{len(df_engineered)}')\n",
        "\n",
        "        print('\\nLEAKAGE-PROOF FEATURES IMPACT:')\n",
        "        print('   Enterprise-grade GDP forecasting with:')\n",
        "        print('   - No future data leakage')\n",
        "        print('   - Proper temporal validation')\n",
        "        print('   - Bulletproof audit compliance')\n",
        "        print('   - Production-ready reliability')\n",
        "\n",
        "        return df_engineered, output_filename\n",
        "\n",
        "    except Exception as error:\n",
        "        print(f'Error generating engineered features: {str(error)}')\n",
        "        return None, None\n",
        "\n",
        "\n",
        "# Main execution\n",
        "if df is None:\n",
        "    # Show upload widget\n",
        "    create_data_uploader()\n",
        "else:\n",
        "    # Run feature generation\n",
        "    print('=' * 80)\n",
        "    print('ENGINEERED FEATURES GENERATION')\n",
        "    print('=' * 80)\n",
        "    print()\n",
        "\n",
        "    df_engineered, output_filename = generate_engineered_features()\n",
        "\n",
        "    if df_engineered is not None and output_filename is not None:\n",
        "        print('\\n' + '=' * 80)\n",
        "        print('GENERATION COMPLETE')\n",
        "        print('=' * 80)\n",
        "        print(f'\\nOutput file: {output_filename}')\n",
        "        print('Click the green download button above')"
      ],
      "metadata": {
        "id": "sxLTDl-u5KAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# MODEL TRAINING & GDP FORECAST GENERATOR\n",
        "# =============================================================================\n",
        "\"\"\"Train GDP forecasting model and generate predictions.\n",
        "\n",
        "Loads engineered features from CB3 output file, trains time-series\n",
        "cross-validated model, and generates GDP forecasts.\n",
        "\"\"\"\n",
        "\n",
        "from typing import Optional, Tuple\n",
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from IPython.display import display, HTML\n",
        "import base64\n",
        "import os\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "def load_cb3_data() -> Optional[pd.DataFrame]:\n",
        "    \"\"\"Load data from CB3 output file.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame if file exists and loads successfully, None otherwise.\n",
        "    \"\"\"\n",
        "    cb3_file = 'XLSX#3_Engineered_Features.xlsx'\n",
        "\n",
        "    if os.path.exists(cb3_file):\n",
        "        try:\n",
        "            data = pd.read_excel(cb3_file)\n",
        "            print(f'Loaded: {cb3_file}')\n",
        "            print(f'Shape: {data.shape}')\n",
        "            return data\n",
        "        except Exception as e:\n",
        "            print(f'Error loading {cb3_file}: {e}')\n",
        "            return None\n",
        "    else:\n",
        "        print(f'File not found: {cb3_file}')\n",
        "        print('Please run Code Block 3 first')\n",
        "        return None\n",
        "\n",
        "\n",
        "def train_model(data: pd.DataFrame) -> Tuple[\n",
        "    Optional[pd.DataFrame], Optional[str]\n",
        "]:\n",
        "    \"\"\"Train GDP forecasting model.\n",
        "\n",
        "    Args:\n",
        "        data: DataFrame with base and engineered features.\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (result_dataframe, output_filename).\n",
        "    \"\"\"\n",
        "    print('\\n' + '=' * 80)\n",
        "    print('MODEL TRAINING')\n",
        "    print('=' * 80)\n",
        "\n",
        "    # Define features\n",
        "    features = [\n",
        "        'yield_10yr', 'gas_price', 'jobless_claims_quarterly_avg',\n",
        "        'vix_quarterly_avg', 'CorpYield_QtrAvg', 'UNRATE_QtrAvg',\n",
        "        'GTI_Normalized_0_100', 'gdp_volatility', 'term_spread',\n",
        "        'credit_spread', 'labor_stress', 'financial_stress'\n",
        "    ]\n",
        "\n",
        "    # Check available features\n",
        "    available = [f for f in features if f in data.columns]\n",
        "    missing = [f for f in features if f not in data.columns]\n",
        "\n",
        "    print(f'\\nFeatures: {len(available)}/{len(features)} available')\n",
        "\n",
        "    if missing:\n",
        "        print(f'Missing: {missing}')\n",
        "        print('\\nERROR: Run Code Block 3 to generate engineered features')\n",
        "        return None, None\n",
        "\n",
        "    # Prepare data\n",
        "    X = data[available].dropna()\n",
        "    y = data.loc[X.index, 'gdp_pct_change_target']\n",
        "\n",
        "    print(f'Observations: {len(X)}')\n",
        "\n",
        "    # Cross-validation\n",
        "    print('\\n' + '=' * 80)\n",
        "    print('CROSS-VALIDATION')\n",
        "    print('=' * 80)\n",
        "\n",
        "    tscv = TimeSeriesSplit(n_splits=5, test_size=12)\n",
        "    predictions = np.full(len(y), np.nan)\n",
        "\n",
        "    print('\\n   Fold | Train | Test |   MAE   |  RMSE   | Dir.Acc')\n",
        "    print('   ' + '-' * 55)\n",
        "\n",
        "    for fold, (train_idx, test_idx) in enumerate(tscv.split(X), 1):\n",
        "        pipeline = Pipeline([\n",
        "            ('scaler', StandardScaler()),\n",
        "            ('regressor', LinearRegression())\n",
        "        ])\n",
        "\n",
        "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
        "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
        "\n",
        "        pipeline.fit(X_train, y_train)\n",
        "        y_pred = pipeline.predict(X_test)\n",
        "        predictions[test_idx] = y_pred\n",
        "\n",
        "        mae = np.mean(np.abs(y_test - y_pred))\n",
        "        rmse = np.sqrt(np.mean((y_test - y_pred) ** 2))\n",
        "        dir_acc = np.mean(np.sign(y_test) == np.sign(y_pred)) * 100\n",
        "\n",
        "        print(f'   {fold:4d} | {len(train_idx):5d} | '\n",
        "              f'{len(test_idx):4d} | {mae:7.3f} | '\n",
        "              f'{rmse:7.3f} | {dir_acc:6.1f}%')\n",
        "\n",
        "    # Overall metrics\n",
        "    valid = ~np.isnan(predictions)\n",
        "    mae_cv = np.mean(np.abs(y[valid] - predictions[valid]))\n",
        "    rmse_cv = np.sqrt(np.mean((y[valid] - predictions[valid]) ** 2))\n",
        "    dir_cv = np.mean(np.sign(y[valid]) == np.sign(predictions[valid])) * 100\n",
        "\n",
        "    print(f'\\n   OVERALL: MAE={mae_cv:.3f}, RMSE={rmse_cv:.3f}, '\n",
        "          f'Dir={dir_cv:.1f}%')\n",
        "\n",
        "    # Train final model\n",
        "    print('\\n' + '=' * 80)\n",
        "    print('FINAL MODEL')\n",
        "    print('=' * 80)\n",
        "\n",
        "    final_pipeline = Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('regressor', LinearRegression())\n",
        "    ])\n",
        "\n",
        "    final_pipeline.fit(X, y)\n",
        "    final_preds = final_pipeline.predict(X)\n",
        "\n",
        "    # Add to dataframe\n",
        "    result = data.copy()\n",
        "    result['gdp_forecast'] = np.nan\n",
        "    result.loc[X.index, 'gdp_forecast'] = np.round(final_preds, 2)\n",
        "\n",
        "    print(f'\\nForecasts generated: {len(final_preds)}')\n",
        "\n",
        "    # Save\n",
        "    output_file = 'XLSX#4_GDP_Forecast_Model.xlsx'\n",
        "    result.to_excel(output_file, index=False)\n",
        "\n",
        "    print(f'Saved: {output_file}')\n",
        "\n",
        "    # Auto-download\n",
        "    try:\n",
        "        with open(output_file, 'rb') as f:\n",
        "            b64 = base64.b64encode(f.read()).decode()\n",
        "\n",
        "        size_kb = os.path.getsize(output_file) / 1024\n",
        "\n",
        "        link = (\n",
        "            f'<a href=\"data:application/vnd.openxmlformats-'\n",
        "            f'officedocument.spreadsheetml.sheet;base64,{b64}\" '\n",
        "            f'download=\"{output_file}\" style=\"background-color: '\n",
        "            f'#4CAF50; color: white; padding: 10px 20px; '\n",
        "            f'text-decoration: none; border-radius: 5px; '\n",
        "            f'font-weight: bold;\">DOWNLOAD {output_file} '\n",
        "            f'({size_kb:.1f} KB)</a>'\n",
        "        )\n",
        "\n",
        "        print('\\n')\n",
        "        display(HTML(f'<div style=\"text-align: center; '\n",
        "                    f'margin: 20px;\">{link}</div>'))\n",
        "    except Exception as e:\n",
        "        print(f'\\nDownload failed: {e}')\n",
        "\n",
        "    # Sample results\n",
        "    print('\\nSAMPLE FORECASTS:')\n",
        "    print('   Date        | Actual  | Forecast | Error')\n",
        "    print('   ' + '-' * 47)\n",
        "\n",
        "    for idx in X.index[[0, len(X)//4, len(X)//2, 3*len(X)//4, -1]]:\n",
        "        date = result.loc[idx, 'date']\n",
        "        actual = result.loc[idx, 'gdp_pct_change_target']\n",
        "        forecast = result.loc[idx, 'gdp_forecast']\n",
        "\n",
        "        try:\n",
        "            date_str = pd.to_datetime(date).strftime('%Y-%m-%d')\n",
        "        except:\n",
        "            date_str = str(date)[:10]\n",
        "\n",
        "        print(f'   {date_str} | {actual:+6.2f}% | '\n",
        "              f'{forecast:+7.2f}% | {abs(actual-forecast):5.2f}pp')\n",
        "\n",
        "    print('\\n' + '=' * 80)\n",
        "    print('COMPLETE')\n",
        "    print('=' * 80)\n",
        "\n",
        "    return result, output_file\n",
        "\n",
        "\n",
        "# MAIN EXECUTION\n",
        "print('=' * 80)\n",
        "print('GDP FORECAST MODEL TRAINING')\n",
        "print('=' * 80)\n",
        "\n",
        "# Load data from CB3 output file\n",
        "data = load_cb3_data()\n",
        "\n",
        "if data is not None:\n",
        "    # Train model\n",
        "    result, output = train_model(data)\n",
        "else:\n",
        "    print('\\nCannot proceed without CB3 output file')"
      ],
      "metadata": {
        "id": "m3GgCQjOBILR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# FEATURE IMPORTANCE ANALYSIS (12 FEATURES)\n",
        "# =============================================================================\n",
        "\"\"\"Calculate correlation-based feature importance.\n",
        "\n",
        "Uses the exact same methodology as CB2 (correlation analysis) but now\n",
        "includes the 5 engineered features from CB3.\n",
        "\n",
        "Base features (7): yield_10yr, gas_price, jobless_claims, vix,\n",
        "                   corp_yield, unemployment, geopolitical\n",
        "Engineered features (5): gdp_volatility, term_spread, credit_spread,\n",
        "                         labor_stress, financial_stress\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "import matplotlib.pyplot as plt\n",
        "import base64\n",
        "import os\n",
        "\n",
        "# Load data from CB4\n",
        "print('=' * 80)\n",
        "print('FEATURE IMPORTANCE ANALYSIS (ALL 12 FEATURES)')\n",
        "print('=' * 80)\n",
        "\n",
        "cb4_file = 'XLSX#4_GDP_Forecast_Model.xlsx'\n",
        "\n",
        "if not os.path.exists(cb4_file):\n",
        "    print(f'\\nERROR: {cb4_file} not found')\n",
        "    print('Run Code Block 4 first')\n",
        "else:\n",
        "    # Load data\n",
        "    df = pd.read_excel(cb4_file)\n",
        "    print(f'\\nLoaded: {cb4_file}')\n",
        "    print(f'Shape: {df.shape}')\n",
        "\n",
        "    # Add official GDP column\n",
        "    df['official_gdp_growth'] = df['gdp_pct_change_target']\n",
        "\n",
        "    # Define all features\n",
        "    base_features = [\n",
        "        'yield_10yr',\n",
        "        'gas_price',\n",
        "        'jobless_claims_quarterly_avg',\n",
        "        'vix_quarterly_avg',\n",
        "        'CorpYield_QtrAvg',\n",
        "        'UNRATE_QtrAvg',\n",
        "        'GTI_Normalized_0_100'\n",
        "    ]\n",
        "\n",
        "    engineered_features = [\n",
        "        'gdp_volatility',\n",
        "        'term_spread',\n",
        "        'credit_spread',\n",
        "        'labor_stress',\n",
        "        'financial_stress'\n",
        "    ]\n",
        "\n",
        "    all_features = base_features + engineered_features\n",
        "\n",
        "    # Calculate correlations (EXACT SAME AS CB2)\n",
        "    print('\\n' + '=' * 80)\n",
        "    print('CALCULATING CORRELATIONS WITH GDP GROWTH')\n",
        "    print('=' * 80)\n",
        "\n",
        "    correlations = []\n",
        "\n",
        "    for feature in all_features:\n",
        "        if feature in df.columns:\n",
        "            # Calculate correlation with target\n",
        "            corr = df[feature].corr(df['gdp_pct_change_target'])\n",
        "            correlations.append({\n",
        "                'feature': feature,\n",
        "                'correlation': corr,\n",
        "                'abs_correlation': abs(corr)\n",
        "            })\n",
        "\n",
        "    # Create DataFrame and sort by absolute correlation\n",
        "    corr_df = pd.DataFrame(correlations)\n",
        "    corr_df = corr_df.sort_values('abs_correlation', ascending=False)\n",
        "\n",
        "    # Calculate percentage impact (sum of absolute correlations = 100%)\n",
        "    total_abs_corr = corr_df['abs_correlation'].sum()\n",
        "    corr_df['impact_pct'] = (\n",
        "        corr_df['abs_correlation'] / total_abs_corr * 100\n",
        "    )\n",
        "\n",
        "    # Display results\n",
        "    print('\\nFEATURE IMPORTANCE (CORRELATION-BASED):')\n",
        "    print('Rank | Feature                          | Correlation | Impact')\n",
        "    print('-' * 75)\n",
        "\n",
        "    for idx, row in corr_df.iterrows():\n",
        "        rank = list(corr_df.index).index(idx) + 1\n",
        "        feature = row['feature']\n",
        "        corr = row['correlation']\n",
        "        impact = row['impact_pct']\n",
        "\n",
        "        # Mark if base or engineered\n",
        "        feat_type = 'ENG' if feature in engineered_features else 'BASE'\n",
        "\n",
        "        print(f'{rank:4d} | {feature:32s} | {corr:+11.4f} | '\n",
        "              f'{impact:6.2f}% [{feat_type}]')\n",
        "\n",
        "    print('\\n' + '=' * 80)\n",
        "    print('SUMMARY')\n",
        "    print('=' * 80)\n",
        "\n",
        "    base_impact = corr_df[\n",
        "        corr_df['feature'].isin(base_features)\n",
        "    ]['impact_pct'].sum()\n",
        "\n",
        "    eng_impact = corr_df[\n",
        "        corr_df['feature'].isin(engineered_features)\n",
        "    ]['impact_pct'].sum()\n",
        "\n",
        "    print(f'\\nBase features (7):       {base_impact:.1f}%')\n",
        "    print(f'Engineered features (5): {eng_impact:.1f}%')\n",
        "    print(f'Total:                   100.0%')\n",
        "\n",
        "    # Top 5 features\n",
        "    print('\\nTOP 5 MOST IMPORTANT FEATURES:')\n",
        "    for idx, row in corr_df.head(5).iterrows():\n",
        "        rank = list(corr_df.index).index(idx) + 1\n",
        "        feature = row['feature']\n",
        "        impact = row['impact_pct']\n",
        "        print(f'   {rank}. {feature}: {impact:.2f}%')\n",
        "\n",
        "    # Save results\n",
        "    output_file = 'XLSX#5_Feature_Importance.xlsx'\n",
        "\n",
        "    # Add correlation data to output\n",
        "    result_df = df.copy()\n",
        "\n",
        "    # Add feature importance as new columns\n",
        "    for _, row in corr_df.iterrows():\n",
        "        feat = row['feature']\n",
        "        result_df[f'{feat}_importance'] = row['impact_pct']\n",
        "\n",
        "    result_df.to_excel(output_file, index=False)\n",
        "\n",
        "    print(f'\\n' + '=' * 80)\n",
        "    print(f'SAVED: {output_file}')\n",
        "    print('=' * 80)\n",
        "\n",
        "    # Auto-download\n",
        "    try:\n",
        "        with open(output_file, 'rb') as f:\n",
        "            b64 = base64.b64encode(f.read()).decode()\n",
        "\n",
        "        size_kb = os.path.getsize(output_file) / 1024\n",
        "\n",
        "        link = (\n",
        "            f'<a href=\"data:application/vnd.openxmlformats-'\n",
        "            f'officedocument.spreadsheetml.sheet;base64,{b64}\" '\n",
        "            f'download=\"{output_file}\" style=\"background-color: '\n",
        "            f'#4CAF50; color: white; padding: 10px 20px; '\n",
        "            f'text-decoration: none; border-radius: 5px; '\n",
        "            f'font-weight: bold;\">DOWNLOAD {output_file} '\n",
        "            f'({size_kb:.1f} KB)</a>'\n",
        "        )\n",
        "\n",
        "        print('\\n')\n",
        "        display(HTML(f'<div style=\"text-align: center; '\n",
        "                    f'margin: 20px;\">{link}</div>'))\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    print('\\n' + '=' * 80)\n",
        "    print('COMPLETE')\n",
        "    print('=' * 80)"
      ],
      "metadata": {
        "id": "OHoIIpj3FmGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CLEAN GDP FORECAST OUTPUT WITH BENCHMARK COMPARISON\n",
        "# =============================================================================\n",
        "\"\"\"Generate accurate forecast output with RMSE calculation.\n",
        "\n",
        "Excludes incomplete 2025-10-01 data from metrics.\n",
        "Compares performance to Atlanta Fed GDPNow benchmark.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from IPython.display import display, HTML\n",
        "import base64\n",
        "import os\n",
        "\n",
        "print('=' * 80)\n",
        "print('GENERATING GDP FORECAST OUTPUT')\n",
        "print('=' * 80)\n",
        "\n",
        "# Load data from CB4\n",
        "cb4_file = 'XLSX#4_GDP_Forecast_Model.xlsx'\n",
        "\n",
        "if os.path.exists(cb4_file):\n",
        "    df = pd.read_excel(cb4_file)\n",
        "    print(f'\\nLoaded: {cb4_file}')\n",
        "\n",
        "    # Define features\n",
        "    features = [\n",
        "        'yield_10yr', 'gas_price', 'jobless_claims_quarterly_avg',\n",
        "        'vix_quarterly_avg', 'CorpYield_QtrAvg', 'UNRATE_QtrAvg',\n",
        "        'GTI_Normalized_0_100', 'gdp_volatility', 'term_spread',\n",
        "        'credit_spread', 'labor_stress', 'financial_stress'\n",
        "    ]\n",
        "\n",
        "    # Prepare data\n",
        "    available = [f for f in features if f in df.columns]\n",
        "    X = df[available].dropna()\n",
        "    y = df.loc[X.index, 'gdp_pct_change_target']\n",
        "\n",
        "    # Filter out 2025-10-01 and later\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "    cutoff_date = pd.to_datetime('2025-10-01')\n",
        "    valid_mask = df['date'] < cutoff_date\n",
        "    valid_indices = X.index.intersection(df[valid_mask].index)\n",
        "    excluded_count = len(X) - len(valid_indices)\n",
        "\n",
        "    print(f'\\nExcluded >= 2025-10-01: {excluded_count} rows')\n",
        "    print(f'Used for RMSE: {len(valid_indices)} rows')\n",
        "\n",
        "    # Train model\n",
        "    pipeline = Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('regressor', LinearRegression())\n",
        "    ])\n",
        "\n",
        "    pipeline.fit(X, y)\n",
        "    predictions_all = pipeline.predict(X)\n",
        "\n",
        "    # Calculate metrics on valid data only\n",
        "    y_valid = y.loc[valid_indices]\n",
        "    pred_valid = predictions_all[X.index.get_indexer(valid_indices)]\n",
        "\n",
        "    errors = y_valid - pred_valid\n",
        "    squared_errors = errors ** 2\n",
        "\n",
        "    mae = np.mean(np.abs(errors))\n",
        "    mse = np.mean(squared_errors)\n",
        "    rmse = np.sqrt(mse)\n",
        "    sum_squared_errors = np.sum(squared_errors)\n",
        "\n",
        "    print(f'\\nRMSE: {rmse:.2f} pp')\n",
        "    print(f'MAE:  {mae:.2f} pp')\n",
        "\n",
        "    # Create output dataframe\n",
        "    output_df = df.copy()\n",
        "\n",
        "    # Round all numeric columns to 2 decimals\n",
        "    numeric_cols = output_df.select_dtypes(include=[np.number]).columns\n",
        "    output_df[numeric_cols] = output_df[numeric_cols].round(2)\n",
        "\n",
        "    # Add official GDP growth\n",
        "    output_df['official_gdp_growth'] = output_df['gdp_pct_change_target']\n",
        "\n",
        "    # Add forecast\n",
        "    output_df['gdp_forecast'] = np.nan\n",
        "    output_df.loc[X.index, 'gdp_forecast'] = np.round(predictions_all, 2)\n",
        "\n",
        "    # Add forecast error\n",
        "    output_df['forecast_error'] = np.nan\n",
        "    output_df.loc[X.index, 'forecast_error'] = np.round(\n",
        "        output_df.loc[X.index, 'official_gdp_growth'] -\n",
        "        output_df.loc[X.index, 'gdp_forecast'], 2\n",
        "    )\n",
        "\n",
        "    # Add squared error\n",
        "    output_df['squared_error'] = np.nan\n",
        "    output_df.loc[X.index, 'squared_error'] = np.round(\n",
        "        (output_df.loc[X.index, 'forecast_error']) ** 2, 2\n",
        "    )\n",
        "\n",
        "    # RMSE calculation box at bottom\n",
        "    last_row = len(output_df)\n",
        "\n",
        "    # Blank rows\n",
        "    for i in range(3):\n",
        "        output_df.loc[last_row + i] = [np.nan] * len(output_df.columns)\n",
        "\n",
        "    calc_start = last_row + 3\n",
        "\n",
        "    # Header\n",
        "    output_df.loc[calc_start, 'date'] = '═══════════════════════════════════════'\n",
        "    output_df.loc[calc_start + 1, 'date'] = 'MODEL PERFORMANCE METRICS'\n",
        "    output_df.loc[calc_start + 2, 'date'] = '═══════════════════════════════════════'\n",
        "\n",
        "    # Data info\n",
        "    output_df.loc[calc_start + 3, 'date'] = 'Observations Used:'\n",
        "    output_df.loc[calc_start + 3, 'yield_10yr'] = len(valid_indices)\n",
        "\n",
        "    output_df.loc[calc_start + 4, 'date'] = 'Excluded (>= 2025-10-01):'\n",
        "    output_df.loc[calc_start + 4, 'yield_10yr'] = excluded_count\n",
        "\n",
        "    output_df.loc[calc_start + 5, 'date'] = ''\n",
        "\n",
        "    # RMSE calculation\n",
        "    output_df.loc[calc_start + 6, 'date'] = 'RMSE CALCULATION:'\n",
        "    output_df.loc[calc_start + 7, 'date'] = 'Formula:'\n",
        "    output_df.loc[calc_start + 7, 'yield_10yr'] = 'RMSE = √(Σ(actual - forecast)² / n)'\n",
        "\n",
        "    output_df.loc[calc_start + 8, 'date'] = 'Sum of squared errors:'\n",
        "    output_df.loc[calc_start + 8, 'yield_10yr'] = f'{sum_squared_errors:.2f}'\n",
        "\n",
        "    output_df.loc[calc_start + 9, 'date'] = 'Divide by n:'\n",
        "    output_df.loc[calc_start + 9, 'yield_10yr'] = f'{sum_squared_errors:.2f} ÷ {len(valid_indices)} = {mse:.2f}'\n",
        "\n",
        "    output_df.loc[calc_start + 10, 'date'] = 'Take square root:'\n",
        "    output_df.loc[calc_start + 10, 'yield_10yr'] = f'√{mse:.2f} = {rmse:.2f}'\n",
        "\n",
        "    output_df.loc[calc_start + 11, 'date'] = ''\n",
        "\n",
        "    # Final RMSE\n",
        "    output_df.loc[calc_start + 12, 'date'] = '═══════════════════════════════════════'\n",
        "    output_df.loc[calc_start + 13, 'date'] = '>>> OUR MODEL RMSE:'\n",
        "    output_df.loc[calc_start + 13, 'yield_10yr'] = f'{rmse:.2f} percentage points'\n",
        "    output_df.loc[calc_start + 14, 'date'] = '═══════════════════════════════════════'\n",
        "\n",
        "    # Benchmark\n",
        "    output_df.loc[calc_start + 15, 'date'] = ''\n",
        "    output_df.loc[calc_start + 16, 'date'] = 'BENCHMARK COMPARISON:'\n",
        "    output_df.loc[calc_start + 17, 'date'] = 'Atlanta Fed GDPNow (2000-2013):'\n",
        "    output_df.loc[calc_start + 17, 'yield_10yr'] = '1.15 pp'\n",
        "    output_df.loc[calc_start + 18, 'date'] = 'Our Model:'\n",
        "    output_df.loc[calc_start + 18, 'yield_10yr'] = f'{rmse:.2f} pp'\n",
        "\n",
        "    output_df.loc[calc_start + 19, 'date'] = ''\n",
        "    output_df.loc[calc_start + 20, 'date'] = 'MAE:'\n",
        "    output_df.loc[calc_start + 20, 'yield_10yr'] = f'{mae:.2f} pp'\n",
        "\n",
        "    # Save\n",
        "    output_file = 'XLSX#6_GDP_Benchmark.xlsx'\n",
        "    output_df.to_excel(output_file, index=False)\n",
        "\n",
        "    print(f'\\nSaved: {output_file}')\n",
        "\n",
        "    # Auto-download\n",
        "    try:\n",
        "        with open(output_file, 'rb') as f:\n",
        "            b64 = base64.b64encode(f.read()).decode()\n",
        "\n",
        "        size_kb = os.path.getsize(output_file) / 1024\n",
        "\n",
        "        link = (\n",
        "            f'<a href=\"data:application/vnd.openxmlformats-'\n",
        "            f'officedocument.spreadsheetml.sheet;base64,{b64}\" '\n",
        "            f'download=\"{output_file}\" style=\"background-color: '\n",
        "            f'#4CAF50; color: white; padding: 10px 20px; '\n",
        "            f'text-decoration: none; border-radius: 5px; '\n",
        "            f'font-weight: bold;\">DOWNLOAD {output_file} '\n",
        "            f'({size_kb:.1f} KB)</a>'\n",
        "        )\n",
        "\n",
        "        print('\\n')\n",
        "        display(HTML(f'<div style=\"text-align: center; '\n",
        "                    f'margin: 20px;\">{link}</div>'))\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    print('\\n' + '=' * 80)\n",
        "    print('COMPLETE')\n",
        "    print('=' * 80)\n",
        "\n",
        "else:\n",
        "    print(f'\\nERROR: {cb4_file} not found')"
      ],
      "metadata": {
        "id": "p1EdNtqwaqbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# AUTOREGRESSIVE FEATURES FOR GDP FORECASTING\n",
        "# =============================================================================\n",
        "\"\"\"Add autoregressive GDP features to improve accuracy.\n",
        "\n",
        "Adds lagged GDP values and moving averages to capture GDP persistence\n",
        "and momentum. These are the most powerful predictors for GDP nowcasting.\n",
        "\n",
        "New features:\n",
        "- gdp_lag1, gdp_lag2, gdp_lag3, gdp_lag4: Past GDP values\n",
        "- gdp_ma2, gdp_ma4: Moving averages of past GDP\n",
        "- gdp_momentum: gdp_lag1 - gdp_lag4 (trend direction)\n",
        "- gdp_acceleration: gdp_lag1 - gdp_lag2 (change in trend)\n",
        "\n",
        "All features use shift(1) to prevent data leakage.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from IPython.display import display, HTML\n",
        "import base64\n",
        "import os\n",
        "\n",
        "print('=' * 80)\n",
        "print('AUTOREGRESSIVE GDP FEATURES')\n",
        "print('=' * 80)\n",
        "\n",
        "# Load data from CB4\n",
        "cb4_file = 'XLSX#4_GDP_Forecast_Model.xlsx'\n",
        "\n",
        "if os.path.exists(cb4_file):\n",
        "    df = pd.read_excel(cb4_file)\n",
        "    print(f'\\nLoaded: {cb4_file}')\n",
        "    print(f'Rows: {len(df)}')\n",
        "\n",
        "    # ENGINEER AUTOREGRESSIVE FEATURES\n",
        "    print('\\n' + '=' * 80)\n",
        "    print('CREATING AUTOREGRESSIVE FEATURES')\n",
        "    print('=' * 80)\n",
        "\n",
        "    # Lag features (shift by 1 to prevent leakage)\n",
        "    df['gdp_lag1'] = df['gdp_pct_change_target'].shift(1)\n",
        "    df['gdp_lag2'] = df['gdp_pct_change_target'].shift(2)\n",
        "    df['gdp_lag3'] = df['gdp_pct_change_target'].shift(3)\n",
        "    df['gdp_lag4'] = df['gdp_pct_change_target'].shift(4)\n",
        "\n",
        "    # Moving averages (of past GDP values)\n",
        "    df['gdp_ma2'] = df['gdp_pct_change_target'].shift(1).rolling(\n",
        "        window=2, min_periods=1\n",
        "    ).mean()\n",
        "    df['gdp_ma4'] = df['gdp_pct_change_target'].shift(1).rolling(\n",
        "        window=4, min_periods=2\n",
        "    ).mean()\n",
        "\n",
        "    # Momentum and acceleration\n",
        "    df['gdp_momentum'] = df['gdp_lag1'] - df['gdp_lag4']\n",
        "    df['gdp_acceleration'] = df['gdp_lag1'] - df['gdp_lag2']\n",
        "\n",
        "    print('\\nAdded 8 autoregressive features:')\n",
        "    print('   - gdp_lag1, gdp_lag2, gdp_lag3, gdp_lag4')\n",
        "    print('   - gdp_ma2, gdp_ma4')\n",
        "    print('   - gdp_momentum, gdp_acceleration')\n",
        "    print('\\n   All use shift(1) - NO DATA LEAKAGE')\n",
        "\n",
        "    # Define ALL features\n",
        "    base_features = [\n",
        "        'yield_10yr', 'gas_price', 'jobless_claims_quarterly_avg',\n",
        "        'vix_quarterly_avg', 'CorpYield_QtrAvg', 'UNRATE_QtrAvg',\n",
        "        'GTI_Normalized_0_100'\n",
        "    ]\n",
        "\n",
        "    engineered_features = [\n",
        "        'gdp_volatility', 'term_spread', 'credit_spread',\n",
        "        'labor_stress', 'financial_stress'\n",
        "    ]\n",
        "\n",
        "    autoregressive_features = [\n",
        "        'gdp_lag1', 'gdp_lag2', 'gdp_lag3', 'gdp_lag4',\n",
        "        'gdp_ma2', 'gdp_ma4', 'gdp_momentum', 'gdp_acceleration'\n",
        "    ]\n",
        "\n",
        "    all_features = base_features + engineered_features + autoregressive_features\n",
        "\n",
        "    # Prepare data\n",
        "    available = [f for f in all_features if f in df.columns]\n",
        "\n",
        "    print(f'\\nTotal features: {len(available)}')\n",
        "    print(f'   Base: {len(base_features)}')\n",
        "    print(f'   Engineered: {len(engineered_features)}')\n",
        "    print(f'   Autoregressive: {len([f for f in autoregressive_features if f in available])}')\n",
        "\n",
        "    X = df[available].dropna()\n",
        "    y = df.loc[X.index, 'gdp_pct_change_target']\n",
        "\n",
        "    # Filter out 2025-10-01 and later\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "    cutoff_date = pd.to_datetime('2025-10-01')\n",
        "    valid_mask = df['date'] < cutoff_date\n",
        "    valid_indices = X.index.intersection(df[valid_mask].index)\n",
        "    excluded_count = len(X) - len(valid_indices)\n",
        "\n",
        "    print(f'\\nData after adding lags:')\n",
        "    print(f'   Total observations: {len(X)}')\n",
        "    print(f'   Excluded (>= 2025-10-01): {excluded_count}')\n",
        "    print(f'   Used for RMSE: {len(valid_indices)}')\n",
        "\n",
        "    # Train model\n",
        "    print('\\n' + '=' * 80)\n",
        "    print('TRAINING MODEL WITH AUTOREGRESSIVE FEATURES')\n",
        "    print('=' * 80)\n",
        "\n",
        "    pipeline = Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('regressor', LinearRegression())\n",
        "    ])\n",
        "\n",
        "    pipeline.fit(X, y)\n",
        "    predictions_all = pipeline.predict(X)\n",
        "\n",
        "    # Calculate metrics on valid data only\n",
        "    y_valid = y.loc[valid_indices]\n",
        "    pred_valid = predictions_all[X.index.get_indexer(valid_indices)]\n",
        "\n",
        "    errors = y_valid - pred_valid\n",
        "    squared_errors = errors ** 2\n",
        "\n",
        "    rmse = np.sqrt(np.mean(squared_errors))\n",
        "    sum_squared_errors = np.sum(squared_errors)\n",
        "    dir_acc = np.mean(np.sign(y_valid) == np.sign(pred_valid)) * 100\n",
        "\n",
        "    print(f'\\nPERFORMANCE:')\n",
        "    print(f'   RMSE: {rmse:.2f} pp')\n",
        "    print(f'   Directional Accuracy: {dir_acc:.1f}%')\n",
        "\n",
        "    # Compare to CB5\n",
        "    print('\\n' + '=' * 80)\n",
        "    print('IMPROVEMENT FROM CB5')\n",
        "    print('=' * 80)\n",
        "    print(f'   CB5 (without AR features): 0.94 pp')\n",
        "    print(f'   CB6 (with AR features):    {rmse:.2f} pp')\n",
        "\n",
        "    improvement = ((0.94 - rmse) / 0.94) * 100\n",
        "    print(f'   Improvement: {improvement:.1f}%')\n",
        "\n",
        "    # Create output dataframe\n",
        "    output_df = df.copy()\n",
        "\n",
        "    # Round all numeric columns to 2 decimals\n",
        "    numeric_cols = output_df.select_dtypes(include=[np.number]).columns\n",
        "    output_df[numeric_cols] = output_df[numeric_cols].round(2)\n",
        "\n",
        "    # Add official GDP growth\n",
        "    output_df['official_gdp_growth'] = output_df['gdp_pct_change_target']\n",
        "\n",
        "    # Add forecast\n",
        "    output_df['gdp_forecast_ar'] = np.nan\n",
        "    output_df.loc[X.index, 'gdp_forecast_ar'] = np.round(predictions_all, 2)\n",
        "\n",
        "    # Add forecast error\n",
        "    output_df['forecast_error'] = np.nan\n",
        "    output_df.loc[X.index, 'forecast_error'] = np.round(\n",
        "        output_df.loc[X.index, 'official_gdp_growth'] -\n",
        "        output_df.loc[X.index, 'gdp_forecast_ar'], 2\n",
        "    )\n",
        "\n",
        "    # Add squared error\n",
        "    output_df['squared_error'] = np.nan\n",
        "    output_df.loc[X.index, 'squared_error'] = np.round(\n",
        "        (output_df.loc[X.index, 'forecast_error']) ** 2, 2\n",
        "    )\n",
        "\n",
        "    # RMSE calculation box at bottom\n",
        "    last_row = len(output_df)\n",
        "\n",
        "    # Blank rows\n",
        "    for i in range(3):\n",
        "        output_df.loc[last_row + i] = [np.nan] * len(output_df.columns)\n",
        "\n",
        "    calc_start = last_row + 3\n",
        "\n",
        "    # Header\n",
        "    output_df.loc[calc_start, 'date'] = '═══════════════════════════════════════'\n",
        "    output_df.loc[calc_start + 1, 'date'] = 'MODEL PERFORMANCE METRICS'\n",
        "    output_df.loc[calc_start + 2, 'date'] = '═══════════════════════════════════════'\n",
        "\n",
        "    # Data info\n",
        "    output_df.loc[calc_start + 3, 'date'] = 'Features Used:'\n",
        "    output_df.loc[calc_start + 3, 'yield_10yr'] = f'{len(available)} (7 base + 5 eng + 8 AR)'\n",
        "\n",
        "    output_df.loc[calc_start + 4, 'date'] = 'Observations:'\n",
        "    output_df.loc[calc_start + 4, 'yield_10yr'] = len(valid_indices)\n",
        "\n",
        "    output_df.loc[calc_start + 5, 'date'] = 'Excluded (>= 2025-10-01):'\n",
        "    output_df.loc[calc_start + 5, 'yield_10yr'] = excluded_count\n",
        "\n",
        "    output_df.loc[calc_start + 6, 'date'] = ''\n",
        "\n",
        "    # RMSE calculation\n",
        "    output_df.loc[calc_start + 7, 'date'] = 'RMSE CALCULATION:'\n",
        "    output_df.loc[calc_start + 8, 'date'] = 'Formula:'\n",
        "    output_df.loc[calc_start + 8, 'yield_10yr'] = 'RMSE = √(Σ(actual - forecast)² / n)'\n",
        "\n",
        "    output_df.loc[calc_start + 9, 'date'] = 'Sum of squared errors:'\n",
        "    output_df.loc[calc_start + 9, 'yield_10yr'] = f'{sum_squared_errors:.2f}'\n",
        "\n",
        "    output_df.loc[calc_start + 10, 'date'] = 'Divide by n:'\n",
        "    output_df.loc[calc_start + 10, 'yield_10yr'] = f'{sum_squared_errors:.2f} ÷ {len(valid_indices)} = {np.mean(squared_errors):.2f}'\n",
        "\n",
        "    output_df.loc[calc_start + 11, 'date'] = 'Take square root:'\n",
        "    output_df.loc[calc_start + 11, 'yield_10yr'] = f'√{np.mean(squared_errors):.2f} = {rmse:.2f}'\n",
        "\n",
        "    output_df.loc[calc_start + 12, 'date'] = ''\n",
        "\n",
        "    # Final RMSE\n",
        "    output_df.loc[calc_start + 13, 'date'] = '═══════════════════════════════════════'\n",
        "    output_df.loc[calc_start + 14, 'date'] = '>>> RMSE WITH AR FEATURES:'\n",
        "    output_df.loc[calc_start + 14, 'yield_10yr'] = f'{rmse:.2f} pp'\n",
        "    output_df.loc[calc_start + 15, 'date'] = '═══════════════════════════════════════'\n",
        "\n",
        "    # Comparison\n",
        "    output_df.loc[calc_start + 16, 'date'] = ''\n",
        "    output_df.loc[calc_start + 17, 'date'] = 'PERFORMANCE COMPARISON:'\n",
        "    output_df.loc[calc_start + 18, 'date'] = 'Atlanta Fed GDPNow:'\n",
        "    output_df.loc[calc_start + 18, 'yield_10yr'] = '1.15 pp'\n",
        "    output_df.loc[calc_start + 19, 'date'] = 'CB5 (without AR):'\n",
        "    output_df.loc[calc_start + 19, 'yield_10yr'] = '0.94 pp'\n",
        "    output_df.loc[calc_start + 20, 'date'] = 'CB6 (with AR):'\n",
        "    output_df.loc[calc_start + 20, 'yield_10yr'] = f'{rmse:.2f} pp'\n",
        "    output_df.loc[calc_start + 21, 'date'] = 'Improvement:'\n",
        "    output_df.loc[calc_start + 21, 'yield_10yr'] = f'{improvement:.1f}%'\n",
        "\n",
        "    # Save\n",
        "    output_file = 'XLSX#7_Autoregressive_Features.xlsx'\n",
        "    output_df.to_excel(output_file, index=False)\n",
        "\n",
        "    print(f'\\n' + '=' * 80)\n",
        "    print(f'SAVED: {output_file}')\n",
        "    print('=' * 80)\n",
        "\n",
        "    # Auto-download\n",
        "    try:\n",
        "        with open(output_file, 'rb') as f:\n",
        "            b64 = base64.b64encode(f.read()).decode()\n",
        "\n",
        "        size_kb = os.path.getsize(output_file) / 1024\n",
        "\n",
        "        link = (\n",
        "            f'<a href=\"data:application/vnd.openxmlformats-'\n",
        "            f'officedocument.spreadsheetml.sheet;base64,{b64}\" '\n",
        "            f'download=\"{output_file}\" style=\"background-color: '\n",
        "            f'#4CAF50; color: white; padding: 10px 20px; '\n",
        "            f'text-decoration: none; border-radius: 5px; '\n",
        "            f'font-weight: bold;\">DOWNLOAD {output_file} '\n",
        "            f'({size_kb:.1f} KB)</a>'\n",
        "        )\n",
        "\n",
        "        print('\\n')\n",
        "        display(HTML(f'<div style=\"text-align: center; '\n",
        "                    f'margin: 20px;\">{link}</div>'))\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    print('\\n' + '=' * 80)\n",
        "    print('CB6 COMPLETE')\n",
        "    print('=' * 80)\n",
        "    print(f'\\nYour GDP Forecast in this final XLSX')\n",
        "\n",
        "else:\n",
        "    print(f'\\nERROR: {cb4_file} not found')\n",
        "\n",
        "    # RMSE calculation box at bottom\n",
        "    last_row = len(output_df)\n",
        "\n",
        "    # Add 3 blank rows for spacing\n",
        "    for i in range(3):\n",
        "        output_df.loc[last_row + i] = [np.nan] * len(output_df.columns)\n",
        "\n",
        "    calc_start = last_row + 3\n",
        "\n",
        "    # Box Header\n",
        "    output_df.loc[calc_start, 'date'] = '═══════════════════════════════════════'\n",
        "    output_df.loc[calc_start + 1, 'date'] = 'MODEL PERFORMANCE METRICS - CB6'\n",
        "    output_df.loc[calc_start + 2, 'date'] = '═══════════════════════════════════════'\n",
        "\n",
        "    # Features info\n",
        "    output_df.loc[calc_start + 3, 'date'] = 'Features Used:'\n",
        "    output_df.loc[calc_start + 3, 'yield_10yr'] = f'{len(available)} (7 base + 5 eng + 8 AR)'\n",
        "\n",
        "    output_df.loc[calc_start + 4, 'date'] = 'Observations:'\n",
        "    output_df.loc[calc_start + 4, 'yield_10yr'] = len(valid_indices)\n",
        "\n",
        "    output_df.loc[calc_start + 5, 'date'] = ''\n",
        "\n",
        "    # RMSE calculation\n",
        "    output_df.loc[calc_start + 6, 'date'] = 'RMSE CALCULATION:'\n",
        "    output_df.loc[calc_start + 7, 'date'] = 'Sum of squared errors:'\n",
        "    output_df.loc[calc_start + 7, 'yield_10yr'] = f'{sum_squared_errors:.2f}'\n",
        "\n",
        "    output_df.loc[calc_start + 8, 'date'] = 'Mean squared error:'\n",
        "    output_df.loc[calc_start + 8, 'yield_10yr'] = f'{np.mean(squared_errors):.2f}'\n",
        "\n",
        "    output_df.loc[calc_start + 9, 'date'] = 'RMSE (√MSE):'\n",
        "    output_df.loc[calc_start + 9, 'yield_10yr'] = f'{rmse:.2f}'\n",
        "\n",
        "    output_df.loc[calc_start + 10, 'date'] = ''\n",
        "\n",
        "    # Final result highlighted\n",
        "    output_df.loc[calc_start + 11, 'date'] = '═══════════════════════════════════════'\n",
        "    output_df.loc[calc_start + 12, 'date'] = '>>> CB6 RMSE:'\n",
        "    output_df.loc[calc_start + 12, 'yield_10yr'] = f'{rmse:.2f} percentage points'\n",
        "    output_df.loc[calc_start + 13, 'date'] = '═══════════════════════════════════════'\n",
        "\n",
        "    # Comparison\n",
        "    output_df.loc[calc_start + 14, 'date'] = ''\n",
        "    output_df.loc[calc_start + 15, 'date'] = 'BENCHMARK COMPARISON:'\n",
        "    output_df.loc[calc_start + 16, 'date'] = 'Atlanta Fed GDPNow:'\n",
        "    output_df.loc[calc_start + 16, 'yield_10yr'] = '1.15 pp'\n",
        "    output_df.loc[calc_start + 17, 'date'] = 'CB5 (no AR features):'\n",
        "    output_df.loc[calc_start + 17, 'yield_10yr'] = '0.94 pp'\n",
        "    output_df.loc[calc_start + 18, 'date'] = 'CB6 (with AR features):'\n",
        "    output_df.loc[calc_start + 18, 'yield_10yr'] = f'{rmse:.2f} pp'\n",
        "\n",
        "    improvement = ((0.94 - rmse) / 0.94) * 100\n",
        "    output_df.loc[calc_start + 19, 'date'] = 'Improvement from CB5:'\n",
        "    output_df.loc[calc_start + 19, 'yield_10yr'] = f'{improvement:.1f}%'"
      ],
      "metadata": {
        "id": "EBiRMu-AeUb_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}